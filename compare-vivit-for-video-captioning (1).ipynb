{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9247869,"sourceType":"datasetVersion","datasetId":5594502}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keras==2.15.0\n!git clone https://github.com/Mfys212/VideoCaptioning.git\n!pip install -r /kaggle/working/VideoCaptioning/requirements.txt","metadata":{"id":"O99VJY12AvRn","trusted":true,"execution":{"iopub.status.busy":"2025-04-29T15:13:20.753354Z","iopub.execute_input":"2025-04-29T15:13:20.753574Z","iopub.status.idle":"2025-04-29T15:13:45.656744Z","shell.execute_reply.started":"2025-04-29T15:13:20.753543Z","shell.execute_reply":"2025-04-29T15:13:45.655938Z"}},"outputs":[{"name":"stdout","text":"Collecting keras==2.15.0\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.4.1\n    Uninstalling keras-3.4.1:\n      Successfully uninstalled keras-3.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\nfatal: destination path 'VideoCaptioning' already exists and is not an empty directory.\nCollecting pycocoevalcap (from -r /kaggle/working/VideoCaptioning/requirements.txt (line 1))\n  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pycocotools>=2.0.2 (from pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1))\n  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap->-r /kaggle/working/VideoCaptioning/requirements.txt (line 1)) (1.16.0)\nDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools, pycocoevalcap\nSuccessfully installed pycocoevalcap-1.2 pycocotools-2.0.8\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from VideoCaptioning import CreateModel\nimport tensorflow as tf\n\nmodel = CreateModel(seed=True, multigpu=True)\nVIDEOS_PATH = \"/kaggle/input/videodata/YouTubeClips/YouTubeClips\"\nFRAMES_STORAGE_PATH = \"/kaggle/working/extracted_frames\"\nCAPTIONS_PATH = \"/kaggle/input/videodata/MSVD-indonesian.txt\"\nD_MODELS = 512\nSEQ_LENGTH = 40\nSPATIAL_SIZE = 224\nMAX_FRAMES = 16\nNUM_HEADS = 8\nNUM_LAYER = 2\nVOCAB_SIZE = 10000\nEPOCHS = 100\nBATCH_SIZE = 16\nNUM_CAPTIONS = 40","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T15:13:45.657904Z","iopub.execute_input":"2025-04-29T15:13:45.658185Z","iopub.status.idle":"2025-04-29T15:14:00.859229Z","shell.execute_reply.started":"2025-04-29T15:13:45.658144Z","shell.execute_reply":"2025-04-29T15:14:00.858218Z"}},"outputs":[{"name":"stderr","text":"2025-04-29 15:13:48.206429: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-04-29 15:13:48.206563: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-04-29 15:13:48.358596: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Spatio Temporal Attention","metadata":{}},{"cell_type":"code","source":"model.SpatioTemporalAttention(D_MODELS=D_MODELS, \n                              NUM_HEADS=NUM_HEADS, \n                              MAX_FRAMES=MAX_FRAMES, \n                              SPATIAL_SIZE=SPATIAL_SIZE, \n                              VOCAB_SIZE=VOCAB_SIZE, \n                              SEQ_LENGTH=SEQ_LENGTH, \n                              NUM_CAPTIONS=NUM_CAPTIONS,\n                              NUM_L=NUM_LAYER)\n\nhistory1 = model.fit(CAPTIONS_PATH=CAPTIONS_PATH, \n                  VIDEOS_PATH=VIDEOS_PATH, \n                  FRAMES_STORAGE_PATH=FRAMES_STORAGE_PATH, \n                  EPOCHS=EPOCHS, \n                  BATCH_SIZE=BATCH_SIZE,  \n                  test_size=0.2)\nmodel1 = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T15:14:00.861068Z","iopub.execute_input":"2025-04-29T15:14:00.861530Z","iopub.status.idle":"2025-04-30T02:26:01.708098Z","shell.execute_reply.started":"2025-04-29T15:14:00.861507Z","shell.execute_reply":"2025-04-30T02:26:01.707206Z"}},"outputs":[{"name":"stdout","text":"Num of trainable parameters: 19486480\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745939739.501761      98 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"124/124 [==============================] - 439s 3s/step - seq_loss: 357.7809 - seq_acc: 0.0644 - val_seq_loss: 271.3818 - val_seq_acc: 0.1457\nEpoch 2/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 243.0734 - seq_acc: 0.2215 - val_seq_loss: 196.2270 - val_seq_acc: 0.2722\nEpoch 3/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 190.8506 - seq_acc: 0.2894 - val_seq_loss: 174.9693 - val_seq_acc: 0.3110\nEpoch 4/100\n124/124 [==============================] - 406s 3s/step - seq_loss: 172.8928 - seq_acc: 0.3151 - val_seq_loss: 162.1094 - val_seq_acc: 0.3248\nEpoch 5/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 159.6753 - seq_acc: 0.3305 - val_seq_loss: 151.9655 - val_seq_acc: 0.3414\nEpoch 6/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 151.0334 - seq_acc: 0.3411 - val_seq_loss: 143.9793 - val_seq_acc: 0.3494\nEpoch 7/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 143.8935 - seq_acc: 0.3546 - val_seq_loss: 138.9747 - val_seq_acc: 0.3604\nEpoch 8/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 140.2445 - seq_acc: 0.3551 - val_seq_loss: 133.5720 - val_seq_acc: 0.3691\nEpoch 9/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 134.9867 - seq_acc: 0.3623 - val_seq_loss: 128.4451 - val_seq_acc: 0.3781\nEpoch 10/100\n124/124 [==============================] - 407s 3s/step - seq_loss: 129.1578 - seq_acc: 0.3719 - val_seq_loss: 123.9816 - val_seq_acc: 0.3856\nEpoch 11/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 125.8906 - seq_acc: 0.3837 - val_seq_loss: 119.9798 - val_seq_acc: 0.3930\nEpoch 12/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 121.7960 - seq_acc: 0.3864 - val_seq_loss: 116.4849 - val_seq_acc: 0.3999\nEpoch 13/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 118.7863 - seq_acc: 0.3971 - val_seq_loss: 114.1571 - val_seq_acc: 0.4011\nEpoch 14/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 115.4126 - seq_acc: 0.4035 - val_seq_loss: 109.8586 - val_seq_acc: 0.4175\nEpoch 15/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 113.0849 - seq_acc: 0.4085 - val_seq_loss: 106.2010 - val_seq_acc: 0.4252\nEpoch 16/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 109.8297 - seq_acc: 0.4127 - val_seq_loss: 104.9158 - val_seq_acc: 0.4253\nEpoch 17/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 107.9748 - seq_acc: 0.4202 - val_seq_loss: 103.2988 - val_seq_acc: 0.4245\nEpoch 18/100\n124/124 [==============================] - 406s 3s/step - seq_loss: 104.8691 - seq_acc: 0.4314 - val_seq_loss: 100.1847 - val_seq_acc: 0.4452\nEpoch 19/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 103.1030 - seq_acc: 0.4350 - val_seq_loss: 97.5411 - val_seq_acc: 0.4521\nEpoch 20/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 101.0592 - seq_acc: 0.4404 - val_seq_loss: 94.4190 - val_seq_acc: 0.4669\nEpoch 21/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 98.9661 - seq_acc: 0.4501 - val_seq_loss: 93.3913 - val_seq_acc: 0.4727\nEpoch 22/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 96.3402 - seq_acc: 0.4630 - val_seq_loss: 89.9380 - val_seq_acc: 0.4817\nEpoch 23/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 93.3004 - seq_acc: 0.4724 - val_seq_loss: 86.5997 - val_seq_acc: 0.4998\nEpoch 24/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 91.0124 - seq_acc: 0.4834 - val_seq_loss: 86.1009 - val_seq_acc: 0.4998\nEpoch 25/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 89.2583 - seq_acc: 0.4905 - val_seq_loss: 83.8169 - val_seq_acc: 0.5121\nEpoch 26/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 86.8000 - seq_acc: 0.4976 - val_seq_loss: 80.7239 - val_seq_acc: 0.5252\nEpoch 27/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 86.0558 - seq_acc: 0.5023 - val_seq_loss: 79.5501 - val_seq_acc: 0.5332\nEpoch 28/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 83.2817 - seq_acc: 0.5171 - val_seq_loss: 78.0180 - val_seq_acc: 0.5361\nEpoch 29/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 81.2262 - seq_acc: 0.5245 - val_seq_loss: 78.4686 - val_seq_acc: 0.5376\nEpoch 30/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 80.7241 - seq_acc: 0.5267 - val_seq_loss: 75.4082 - val_seq_acc: 0.5483\nEpoch 31/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 79.2296 - seq_acc: 0.5338 - val_seq_loss: 73.1849 - val_seq_acc: 0.5628\nEpoch 32/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 77.4843 - seq_acc: 0.5440 - val_seq_loss: 72.8478 - val_seq_acc: 0.5647\nEpoch 33/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 77.0874 - seq_acc: 0.5437 - val_seq_loss: 71.6226 - val_seq_acc: 0.5689\nEpoch 34/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 74.3297 - seq_acc: 0.5576 - val_seq_loss: 69.6319 - val_seq_acc: 0.5784\nEpoch 35/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 72.7355 - seq_acc: 0.5637 - val_seq_loss: 68.3441 - val_seq_acc: 0.5829\nEpoch 36/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 71.3454 - seq_acc: 0.5712 - val_seq_loss: 67.2068 - val_seq_acc: 0.5892\nEpoch 37/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 69.8569 - seq_acc: 0.5779 - val_seq_loss: 64.3351 - val_seq_acc: 0.6033\nEpoch 38/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 66.6726 - seq_acc: 0.5925 - val_seq_loss: 62.6730 - val_seq_acc: 0.6092\nEpoch 39/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 65.4174 - seq_acc: 0.5980 - val_seq_loss: 61.4953 - val_seq_acc: 0.6160\nEpoch 40/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 63.4805 - seq_acc: 0.6110 - val_seq_loss: 60.0838 - val_seq_acc: 0.6275\nEpoch 41/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 62.3164 - seq_acc: 0.6110 - val_seq_loss: 57.9117 - val_seq_acc: 0.6339\nEpoch 42/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 60.5412 - seq_acc: 0.6201 - val_seq_loss: 57.5439 - val_seq_acc: 0.6370\nEpoch 43/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 59.7777 - seq_acc: 0.6276 - val_seq_loss: 55.6390 - val_seq_acc: 0.6466\nEpoch 44/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 58.6475 - seq_acc: 0.6295 - val_seq_loss: 54.1203 - val_seq_acc: 0.6523\nEpoch 45/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 57.5503 - seq_acc: 0.6363 - val_seq_loss: 53.6922 - val_seq_acc: 0.6527\nEpoch 46/100\n124/124 [==============================] - 400s 3s/step - seq_loss: 55.6914 - seq_acc: 0.6470 - val_seq_loss: 51.9906 - val_seq_acc: 0.6640\nEpoch 47/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 54.8338 - seq_acc: 0.6505 - val_seq_loss: 51.8271 - val_seq_acc: 0.6645\nEpoch 48/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 53.3283 - seq_acc: 0.6582 - val_seq_loss: 50.0118 - val_seq_acc: 0.6749\nEpoch 49/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 52.2103 - seq_acc: 0.6588 - val_seq_loss: 49.0619 - val_seq_acc: 0.6791\nEpoch 50/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 51.5531 - seq_acc: 0.6643 - val_seq_loss: 48.9003 - val_seq_acc: 0.6799\nEpoch 51/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 50.5559 - seq_acc: 0.6670 - val_seq_loss: 47.0461 - val_seq_acc: 0.6915\nEpoch 52/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 49.0855 - seq_acc: 0.6776 - val_seq_loss: 46.0240 - val_seq_acc: 0.6950\nEpoch 53/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 47.9467 - seq_acc: 0.6812 - val_seq_loss: 47.3108 - val_seq_acc: 0.6871\nEpoch 54/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 48.3080 - seq_acc: 0.6819 - val_seq_loss: 44.4348 - val_seq_acc: 0.7022\nEpoch 55/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 46.7723 - seq_acc: 0.6870 - val_seq_loss: 45.1106 - val_seq_acc: 0.7016\nEpoch 56/100\n124/124 [==============================] - 400s 3s/step - seq_loss: 45.5800 - seq_acc: 0.6963 - val_seq_loss: 45.1118 - val_seq_acc: 0.6984\nEpoch 57/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 45.0114 - seq_acc: 0.6975 - val_seq_loss: 42.0259 - val_seq_acc: 0.7139\nEpoch 58/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 44.2705 - seq_acc: 0.7017 - val_seq_loss: 41.6543 - val_seq_acc: 0.7184\nEpoch 59/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 43.4120 - seq_acc: 0.7068 - val_seq_loss: 41.1927 - val_seq_acc: 0.7207\nEpoch 60/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 42.8701 - seq_acc: 0.7108 - val_seq_loss: 39.9051 - val_seq_acc: 0.7304\nEpoch 61/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 41.8057 - seq_acc: 0.7149 - val_seq_loss: 39.5515 - val_seq_acc: 0.7283\nEpoch 62/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 41.1155 - seq_acc: 0.7174 - val_seq_loss: 39.3399 - val_seq_acc: 0.7300\nEpoch 63/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 41.0226 - seq_acc: 0.7171 - val_seq_loss: 37.9828 - val_seq_acc: 0.7379\nEpoch 64/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 40.1315 - seq_acc: 0.7238 - val_seq_loss: 37.3769 - val_seq_acc: 0.7382\nEpoch 65/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 39.4704 - seq_acc: 0.7255 - val_seq_loss: 39.1341 - val_seq_acc: 0.7307\nEpoch 66/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 39.3473 - seq_acc: 0.7291 - val_seq_loss: 37.1989 - val_seq_acc: 0.7405\nEpoch 67/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 38.3134 - seq_acc: 0.7345 - val_seq_loss: 38.0679 - val_seq_acc: 0.7347\nEpoch 68/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 37.7980 - seq_acc: 0.7363 - val_seq_loss: 36.8211 - val_seq_acc: 0.7411\nEpoch 69/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 37.4648 - seq_acc: 0.7378 - val_seq_loss: 38.6518 - val_seq_acc: 0.7287\nEpoch 70/100\n124/124 [==============================] - 398s 3s/step - seq_loss: 38.5691 - seq_acc: 0.7291 - val_seq_loss: 35.3149 - val_seq_acc: 0.7496\nEpoch 71/100\n124/124 [==============================] - 400s 3s/step - seq_loss: 36.7185 - seq_acc: 0.7418 - val_seq_loss: 35.5764 - val_seq_acc: 0.7476\nEpoch 72/100\n124/124 [==============================] - 400s 3s/step - seq_loss: 36.5172 - seq_acc: 0.7403 - val_seq_loss: 34.0515 - val_seq_acc: 0.7573\nEpoch 73/100\n124/124 [==============================] - 415s 3s/step - seq_loss: 36.1675 - seq_acc: 0.7446 - val_seq_loss: 34.1982 - val_seq_acc: 0.7539\nEpoch 74/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 35.3451 - seq_acc: 0.7493 - val_seq_loss: 35.0046 - val_seq_acc: 0.7486\nEpoch 75/100\n124/124 [==============================] - 409s 3s/step - seq_loss: 35.4733 - seq_acc: 0.7468 - val_seq_loss: 34.1198 - val_seq_acc: 0.7554\nEpoch 76/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 34.8916 - seq_acc: 0.7495 - val_seq_loss: 33.7958 - val_seq_acc: 0.7582\nEpoch 77/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 34.4034 - seq_acc: 0.7541 - val_seq_loss: 32.9978 - val_seq_acc: 0.7603\nEpoch 78/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 33.8668 - seq_acc: 0.7540 - val_seq_loss: 32.4190 - val_seq_acc: 0.7631\nEpoch 79/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 33.8094 - seq_acc: 0.7540 - val_seq_loss: 32.3482 - val_seq_acc: 0.7639\nEpoch 80/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 33.5201 - seq_acc: 0.7554 - val_seq_loss: 32.1599 - val_seq_acc: 0.7643\nEpoch 81/100\n124/124 [==============================] - 406s 3s/step - seq_loss: 33.4459 - seq_acc: 0.7556 - val_seq_loss: 32.2962 - val_seq_acc: 0.7635\nEpoch 82/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 33.0117 - seq_acc: 0.7591 - val_seq_loss: 32.5514 - val_seq_acc: 0.7589\nEpoch 83/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 33.2711 - seq_acc: 0.7568 - val_seq_loss: 33.3456 - val_seq_acc: 0.7556\nEpoch 84/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 32.5865 - seq_acc: 0.7604 - val_seq_loss: 32.3296 - val_seq_acc: 0.7610\nEpoch 85/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 32.3161 - seq_acc: 0.7606 - val_seq_loss: 32.0491 - val_seq_acc: 0.7644\nEpoch 86/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 31.9772 - seq_acc: 0.7631 - val_seq_loss: 31.8823 - val_seq_acc: 0.7596\nEpoch 87/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 32.2053 - seq_acc: 0.7608 - val_seq_loss: 30.7227 - val_seq_acc: 0.7687\nEpoch 88/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 31.5738 - seq_acc: 0.7637 - val_seq_loss: 30.8185 - val_seq_acc: 0.7703\nEpoch 89/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 31.6781 - seq_acc: 0.7642 - val_seq_loss: 32.4298 - val_seq_acc: 0.7559\nEpoch 90/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 31.8275 - seq_acc: 0.7623 - val_seq_loss: 31.4002 - val_seq_acc: 0.7663\nEpoch 91/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 31.0110 - seq_acc: 0.7659 - val_seq_loss: 31.0002 - val_seq_acc: 0.7676\nEpoch 92/100\n124/124 [==============================] - 399s 3s/step - seq_loss: 31.1641 - seq_acc: 0.7661 - val_seq_loss: 31.4990 - val_seq_acc: 0.7641\nEpoch 93/100\n124/124 [==============================] - 399s 3s/step - seq_loss: 31.0289 - seq_acc: 0.7650 - val_seq_loss: 30.4721 - val_seq_acc: 0.7710\nEpoch 94/100\n124/124 [==============================] - 398s 3s/step - seq_loss: 30.7665 - seq_acc: 0.7677 - val_seq_loss: 30.9913 - val_seq_acc: 0.7659\nEpoch 95/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 30.3452 - seq_acc: 0.7683 - val_seq_loss: 30.1715 - val_seq_acc: 0.7735\nEpoch 96/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 30.0304 - seq_acc: 0.7696 - val_seq_loss: 30.4676 - val_seq_acc: 0.7674\nEpoch 97/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 30.3620 - seq_acc: 0.7683 - val_seq_loss: 30.2132 - val_seq_acc: 0.7709\nEpoch 98/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 30.1761 - seq_acc: 0.7699 - val_seq_loss: 30.6710 - val_seq_acc: 0.7667\nEpoch 99/100\n124/124 [==============================] - 400s 3s/step - seq_loss: 29.8693 - seq_acc: 0.7712 - val_seq_loss: 29.0294 - val_seq_acc: 0.7752\nEpoch 100/100\n124/124 [==============================] - 399s 3s/step - seq_loss: 29.8248 - seq_acc: 0.7709 - val_seq_loss: 29.9447 - val_seq_acc: 0.7695\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"acc1, loss1, cider1 = model.eval()\nprint(f\"Accuracy: {acc1}, Loss: {loss1}, CIDEr: {cider1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T02:26:01.709480Z","iopub.execute_input":"2025-04-30T02:26:01.709929Z","iopub.status.idle":"2025-04-30T02:29:49.782541Z","shell.execute_reply.started":"2025-04-30T02:26:01.709894Z","shell.execute_reply":"2025-04-30T02:29:49.781696Z"}},"outputs":[{"name":"stderr","text":"Compute Score: 100%|██████████| 394/394 [03:10<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.769542932510376, Loss: 29.9447021484375, CIDEr: 1.7183912663614491\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Factorised Encoder","metadata":{}},{"cell_type":"code","source":"model.FactorisedEncoder(D_MODELS=D_MODELS, \n                      NUM_HEADS=NUM_HEADS, \n                      MAX_FRAMES=MAX_FRAMES, \n                      SPATIAL_SIZE=SPATIAL_SIZE, \n                      VOCAB_SIZE=VOCAB_SIZE, \n                      SEQ_LENGTH=SEQ_LENGTH, \n                    NUM_CAPTIONS=NUM_CAPTIONS,\n                      NUM_L=NUM_LAYER)\n\nhistory2 = model.fit(CAPTIONS_PATH=CAPTIONS_PATH, \n                  VIDEOS_PATH=VIDEOS_PATH, \n                  FRAMES_STORAGE_PATH=FRAMES_STORAGE_PATH, \n                  EPOCHS=EPOCHS, \n                  BATCH_SIZE=BATCH_SIZE, \n                  test_size=0.2)\nmodel2 = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T11:18:40.422990Z","iopub.execute_input":"2025-04-09T11:18:40.423715Z","iopub.status.idle":"2025-04-09T17:01:59.710615Z","shell.execute_reply.started":"2025-04-09T11:18:40.423677Z","shell.execute_reply":"2025-04-09T17:01:59.709893Z"}},"outputs":[{"name":"stdout","text":"Num of trainable parameters: 22642448\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1744197623.219856      96 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"124/124 [==============================] - 250s 1s/step - seq_loss: 358.8982 - seq_acc: 0.0579 - val_seq_loss: 270.5125 - val_seq_acc: 0.2045\nEpoch 2/100\n124/124 [==============================] - 207s 1s/step - seq_loss: 242.5987 - seq_acc: 0.2293 - val_seq_loss: 195.5726 - val_seq_acc: 0.2876\nEpoch 3/100\n124/124 [==============================] - 207s 1s/step - seq_loss: 191.8041 - seq_acc: 0.2855 - val_seq_loss: 175.8787 - val_seq_acc: 0.2927\nEpoch 4/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 174.1101 - seq_acc: 0.3088 - val_seq_loss: 161.1879 - val_seq_acc: 0.3141\nEpoch 5/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 160.6088 - seq_acc: 0.3297 - val_seq_loss: 151.1730 - val_seq_acc: 0.3459\nEpoch 6/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 152.8565 - seq_acc: 0.3398 - val_seq_loss: 144.2434 - val_seq_acc: 0.3572\nEpoch 7/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 144.8919 - seq_acc: 0.3502 - val_seq_loss: 137.6228 - val_seq_acc: 0.3686\nEpoch 8/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 138.9947 - seq_acc: 0.3602 - val_seq_loss: 132.9389 - val_seq_acc: 0.3784\nEpoch 9/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 134.1794 - seq_acc: 0.3701 - val_seq_loss: 127.4658 - val_seq_acc: 0.3814\nEpoch 10/100\n124/124 [==============================] - 207s 1s/step - seq_loss: 130.9168 - seq_acc: 0.3715 - val_seq_loss: 123.0817 - val_seq_acc: 0.3934\nEpoch 11/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 124.2408 - seq_acc: 0.3865 - val_seq_loss: 119.7280 - val_seq_acc: 0.3991\nEpoch 12/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 122.6772 - seq_acc: 0.3899 - val_seq_loss: 116.0429 - val_seq_acc: 0.4015\nEpoch 13/100\n124/124 [==============================] - 203s 1s/step - seq_loss: 120.0990 - seq_acc: 0.3895 - val_seq_loss: 113.0775 - val_seq_acc: 0.4127\nEpoch 14/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 115.6924 - seq_acc: 0.3995 - val_seq_loss: 110.2005 - val_seq_acc: 0.4138\nEpoch 15/100\n124/124 [==============================] - 203s 1s/step - seq_loss: 113.1034 - seq_acc: 0.4045 - val_seq_loss: 107.8044 - val_seq_acc: 0.4253\nEpoch 16/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 110.1080 - seq_acc: 0.4145 - val_seq_loss: 105.5527 - val_seq_acc: 0.4271\nEpoch 17/100\n124/124 [==============================] - 203s 1s/step - seq_loss: 109.5522 - seq_acc: 0.4171 - val_seq_loss: 102.7011 - val_seq_acc: 0.4375\nEpoch 18/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 105.3066 - seq_acc: 0.4288 - val_seq_loss: 99.8556 - val_seq_acc: 0.4484\nEpoch 19/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 103.7194 - seq_acc: 0.4344 - val_seq_loss: 97.2642 - val_seq_acc: 0.4603\nEpoch 20/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 101.1611 - seq_acc: 0.4428 - val_seq_loss: 95.3882 - val_seq_acc: 0.4694\nEpoch 21/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 98.8071 - seq_acc: 0.4532 - val_seq_loss: 92.7646 - val_seq_acc: 0.4786\nEpoch 22/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 95.6509 - seq_acc: 0.4662 - val_seq_loss: 90.9218 - val_seq_acc: 0.4732\nEpoch 23/100\n124/124 [==============================] - 202s 1s/step - seq_loss: 95.4074 - seq_acc: 0.4655 - val_seq_loss: 89.9439 - val_seq_acc: 0.4687\nEpoch 24/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 93.7054 - seq_acc: 0.4728 - val_seq_loss: 86.5547 - val_seq_acc: 0.5064\nEpoch 25/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 91.2273 - seq_acc: 0.4861 - val_seq_loss: 84.7007 - val_seq_acc: 0.5162\nEpoch 26/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 89.0520 - seq_acc: 0.4953 - val_seq_loss: 82.6773 - val_seq_acc: 0.5240\nEpoch 27/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 86.8959 - seq_acc: 0.5018 - val_seq_loss: 82.2586 - val_seq_acc: 0.5289\nEpoch 28/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 87.2129 - seq_acc: 0.5014 - val_seq_loss: 79.9832 - val_seq_acc: 0.5387\nEpoch 29/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 84.5887 - seq_acc: 0.5136 - val_seq_loss: 78.8723 - val_seq_acc: 0.5415\nEpoch 30/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 84.3367 - seq_acc: 0.5173 - val_seq_loss: 77.3168 - val_seq_acc: 0.5466\nEpoch 31/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 83.9007 - seq_acc: 0.5190 - val_seq_loss: 77.3650 - val_seq_acc: 0.5530\nEpoch 32/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 81.3569 - seq_acc: 0.5305 - val_seq_loss: 76.6949 - val_seq_acc: 0.5578\nEpoch 33/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 80.8441 - seq_acc: 0.5334 - val_seq_loss: 74.0039 - val_seq_acc: 0.5655\nEpoch 34/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 79.3669 - seq_acc: 0.5409 - val_seq_loss: 74.5992 - val_seq_acc: 0.5610\nEpoch 35/100\n124/124 [==============================] - 208s 1s/step - seq_loss: 77.0762 - seq_acc: 0.5507 - val_seq_loss: 71.4043 - val_seq_acc: 0.5786\nEpoch 36/100\n124/124 [==============================] - 210s 1s/step - seq_loss: 75.8561 - seq_acc: 0.5580 - val_seq_loss: 70.0266 - val_seq_acc: 0.5842\nEpoch 37/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 73.4858 - seq_acc: 0.5651 - val_seq_loss: 67.3180 - val_seq_acc: 0.5965\nEpoch 38/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 72.8014 - seq_acc: 0.5698 - val_seq_loss: 66.0429 - val_seq_acc: 0.5998\nEpoch 39/100\n124/124 [==============================] - 209s 1s/step - seq_loss: 70.6848 - seq_acc: 0.5795 - val_seq_loss: 66.4039 - val_seq_acc: 0.6014\nEpoch 40/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 69.7920 - seq_acc: 0.5825 - val_seq_loss: 64.4533 - val_seq_acc: 0.6142\nEpoch 41/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 69.0672 - seq_acc: 0.5884 - val_seq_loss: 63.9037 - val_seq_acc: 0.6173\nEpoch 42/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 68.3557 - seq_acc: 0.5899 - val_seq_loss: 63.1387 - val_seq_acc: 0.6167\nEpoch 43/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 67.1906 - seq_acc: 0.5933 - val_seq_loss: 62.2380 - val_seq_acc: 0.6206\nEpoch 44/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 67.2627 - seq_acc: 0.5968 - val_seq_loss: 59.9923 - val_seq_acc: 0.6348\nEpoch 45/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 64.9796 - seq_acc: 0.6096 - val_seq_loss: 59.7903 - val_seq_acc: 0.6293\nEpoch 46/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 63.5762 - seq_acc: 0.6119 - val_seq_loss: 59.8552 - val_seq_acc: 0.6334\nEpoch 47/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 63.3235 - seq_acc: 0.6160 - val_seq_loss: 58.1225 - val_seq_acc: 0.6429\nEpoch 48/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 63.2622 - seq_acc: 0.6138 - val_seq_loss: 57.6207 - val_seq_acc: 0.6485\nEpoch 49/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 61.2596 - seq_acc: 0.6261 - val_seq_loss: 57.3509 - val_seq_acc: 0.6465\nEpoch 50/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 60.9762 - seq_acc: 0.6252 - val_seq_loss: 56.9375 - val_seq_acc: 0.6493\nEpoch 51/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 59.5736 - seq_acc: 0.6325 - val_seq_loss: 55.7276 - val_seq_acc: 0.6556\nEpoch 52/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 59.2408 - seq_acc: 0.6363 - val_seq_loss: 55.0883 - val_seq_acc: 0.6516\nEpoch 53/100\n124/124 [==============================] - 207s 1s/step - seq_loss: 58.8566 - seq_acc: 0.6356 - val_seq_loss: 54.5166 - val_seq_acc: 0.6582\nEpoch 54/100\n124/124 [==============================] - 207s 1s/step - seq_loss: 57.4080 - seq_acc: 0.6438 - val_seq_loss: 53.3414 - val_seq_acc: 0.6637\nEpoch 55/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 57.2127 - seq_acc: 0.6414 - val_seq_loss: 53.2972 - val_seq_acc: 0.6621\nEpoch 56/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 55.7602 - seq_acc: 0.6490 - val_seq_loss: 52.6083 - val_seq_acc: 0.6679\nEpoch 57/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 55.4457 - seq_acc: 0.6490 - val_seq_loss: 51.6911 - val_seq_acc: 0.6728\nEpoch 58/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 55.1972 - seq_acc: 0.6502 - val_seq_loss: 51.4413 - val_seq_acc: 0.6739\nEpoch 59/100\n124/124 [==============================] - 209s 1s/step - seq_loss: 54.3168 - seq_acc: 0.6528 - val_seq_loss: 51.0071 - val_seq_acc: 0.6748\nEpoch 60/100\n124/124 [==============================] - 211s 1s/step - seq_loss: 53.5939 - seq_acc: 0.6598 - val_seq_loss: 51.0335 - val_seq_acc: 0.6749\nEpoch 61/100\n124/124 [==============================] - 208s 1s/step - seq_loss: 53.1414 - seq_acc: 0.6598 - val_seq_loss: 49.2418 - val_seq_acc: 0.6830\nEpoch 62/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 52.6968 - seq_acc: 0.6618 - val_seq_loss: 49.3177 - val_seq_acc: 0.6839\nEpoch 63/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 51.8392 - seq_acc: 0.6658 - val_seq_loss: 47.9725 - val_seq_acc: 0.6915\nEpoch 64/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 51.3114 - seq_acc: 0.6697 - val_seq_loss: 47.3116 - val_seq_acc: 0.6948\nEpoch 65/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 50.6080 - seq_acc: 0.6733 - val_seq_loss: 47.7081 - val_seq_acc: 0.6892\nEpoch 66/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 50.2540 - seq_acc: 0.6754 - val_seq_loss: 45.9027 - val_seq_acc: 0.6982\nEpoch 67/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 49.4690 - seq_acc: 0.6762 - val_seq_loss: 45.7031 - val_seq_acc: 0.7008\nEpoch 68/100\n124/124 [==============================] - 208s 1s/step - seq_loss: 49.5385 - seq_acc: 0.6740 - val_seq_loss: 44.9933 - val_seq_acc: 0.7040\nEpoch 69/100\n124/124 [==============================] - 207s 1s/step - seq_loss: 48.0603 - seq_acc: 0.6825 - val_seq_loss: 44.4163 - val_seq_acc: 0.7069\nEpoch 70/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 47.6934 - seq_acc: 0.6863 - val_seq_loss: 44.0831 - val_seq_acc: 0.7063\nEpoch 71/100\n124/124 [==============================] - 208s 1s/step - seq_loss: 47.2422 - seq_acc: 0.6872 - val_seq_loss: 43.9417 - val_seq_acc: 0.7107\nEpoch 72/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 46.0323 - seq_acc: 0.6949 - val_seq_loss: 43.3045 - val_seq_acc: 0.7114\nEpoch 73/100\n124/124 [==============================] - 206s 1s/step - seq_loss: 46.0003 - seq_acc: 0.6932 - val_seq_loss: 44.4793 - val_seq_acc: 0.7052\nEpoch 74/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 45.3868 - seq_acc: 0.6959 - val_seq_loss: 42.4771 - val_seq_acc: 0.7185\nEpoch 75/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 44.6755 - seq_acc: 0.6997 - val_seq_loss: 40.9712 - val_seq_acc: 0.7247\nEpoch 76/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 44.7866 - seq_acc: 0.6977 - val_seq_loss: 43.6341 - val_seq_acc: 0.7052\nEpoch 77/100\n124/124 [==============================] - 210s 1s/step - seq_loss: 43.6299 - seq_acc: 0.7032 - val_seq_loss: 41.3047 - val_seq_acc: 0.7214\nEpoch 78/100\n124/124 [==============================] - 207s 1s/step - seq_loss: 43.7700 - seq_acc: 0.7042 - val_seq_loss: 42.7940 - val_seq_acc: 0.7129\nEpoch 79/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 42.7455 - seq_acc: 0.7076 - val_seq_loss: 39.5237 - val_seq_acc: 0.7323\nEpoch 80/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 42.8597 - seq_acc: 0.7088 - val_seq_loss: 40.6003 - val_seq_acc: 0.7252\nEpoch 81/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 42.5504 - seq_acc: 0.7076 - val_seq_loss: 39.0162 - val_seq_acc: 0.7317\nEpoch 82/100\n124/124 [==============================] - 202s 1s/step - seq_loss: 41.8318 - seq_acc: 0.7106 - val_seq_loss: 38.5965 - val_seq_acc: 0.7335\nEpoch 83/100\n124/124 [==============================] - 202s 1s/step - seq_loss: 41.0590 - seq_acc: 0.7157 - val_seq_loss: 41.2171 - val_seq_acc: 0.7194\nEpoch 84/100\n124/124 [==============================] - 203s 1s/step - seq_loss: 40.6805 - seq_acc: 0.7200 - val_seq_loss: 38.5992 - val_seq_acc: 0.7351\nEpoch 85/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 40.1212 - seq_acc: 0.7245 - val_seq_loss: 39.2956 - val_seq_acc: 0.7301\nEpoch 86/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 40.3722 - seq_acc: 0.7194 - val_seq_loss: 37.9239 - val_seq_acc: 0.7329\nEpoch 87/100\n124/124 [==============================] - 202s 1s/step - seq_loss: 39.7636 - seq_acc: 0.7216 - val_seq_loss: 36.5620 - val_seq_acc: 0.7434\nEpoch 88/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 39.3903 - seq_acc: 0.7248 - val_seq_loss: 37.3164 - val_seq_acc: 0.7363\nEpoch 89/100\n124/124 [==============================] - 207s 1s/step - seq_loss: 38.7726 - seq_acc: 0.7278 - val_seq_loss: 38.3384 - val_seq_acc: 0.7333\nEpoch 90/100\n124/124 [==============================] - 207s 1s/step - seq_loss: 38.4942 - seq_acc: 0.7296 - val_seq_loss: 37.6861 - val_seq_acc: 0.7378\nEpoch 91/100\n124/124 [==============================] - 207s 1s/step - seq_loss: 38.7171 - seq_acc: 0.7272 - val_seq_loss: 35.8768 - val_seq_acc: 0.7507\nEpoch 92/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 37.9971 - seq_acc: 0.7323 - val_seq_loss: 36.7034 - val_seq_acc: 0.7420\nEpoch 93/100\n124/124 [==============================] - 205s 1s/step - seq_loss: 37.7170 - seq_acc: 0.7325 - val_seq_loss: 37.4428 - val_seq_acc: 0.7392\nEpoch 94/100\n124/124 [==============================] - 203s 1s/step - seq_loss: 37.9656 - seq_acc: 0.7328 - val_seq_loss: 35.8829 - val_seq_acc: 0.7475\nEpoch 95/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 37.0623 - seq_acc: 0.7373 - val_seq_loss: 35.3757 - val_seq_acc: 0.7468\nEpoch 96/100\n124/124 [==============================] - 203s 1s/step - seq_loss: 36.5753 - seq_acc: 0.7404 - val_seq_loss: 35.4969 - val_seq_acc: 0.7464\nEpoch 97/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 36.9442 - seq_acc: 0.7347 - val_seq_loss: 36.9691 - val_seq_acc: 0.7407\nEpoch 98/100\n124/124 [==============================] - 203s 1s/step - seq_loss: 36.6371 - seq_acc: 0.7376 - val_seq_loss: 36.3733 - val_seq_acc: 0.7424\nEpoch 99/100\n124/124 [==============================] - 203s 1s/step - seq_loss: 36.2659 - seq_acc: 0.7405 - val_seq_loss: 34.2366 - val_seq_acc: 0.7556\nEpoch 100/100\n124/124 [==============================] - 204s 1s/step - seq_loss: 35.8664 - seq_acc: 0.7399 - val_seq_loss: 34.8598 - val_seq_acc: 0.7516\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"acc2, loss2, cider2 = model.eval()\nprint(f\"Accuracy: {acc2}, Loss: {loss2}, CIDEr: {cider2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:01:59.711806Z","iopub.execute_input":"2025-04-09T17:01:59.712106Z","iopub.status.idle":"2025-04-09T17:05:43.605143Z","shell.execute_reply.started":"2025-04-09T17:01:59.712084Z","shell.execute_reply":"2025-04-09T17:05:43.604176Z"}},"outputs":[{"name":"stderr","text":"Compute Score: 100%|██████████| 394/394 [03:18<00:00,  1.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.7516465783119202, Loss: 34.859771728515625, CIDEr: 1.424918807432229\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Factorised Self Attention","metadata":{}},{"cell_type":"code","source":"model.FactorisedSelfAttention(D_MODELS=D_MODELS, \n                              NUM_HEADS=NUM_HEADS, \n                              MAX_FRAMES=MAX_FRAMES, \n                              SPATIAL_SIZE=SPATIAL_SIZE, \n                              VOCAB_SIZE=VOCAB_SIZE, \n                              SEQ_LENGTH=SEQ_LENGTH,\n                              NUM_CAPTIONS=NUM_CAPTIONS,\n                              NUM_L=NUM_LAYER)\n\nhistory3 = model.fit(CAPTIONS_PATH=CAPTIONS_PATH, \n                  VIDEOS_PATH=VIDEOS_PATH, \n                  FRAMES_STORAGE_PATH=FRAMES_STORAGE_PATH, \n                  EPOCHS=EPOCHS, \n                  BATCH_SIZE=BATCH_SIZE, \n                  test_size=0.2)\nmodel3 = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T12:06:43.521856Z","iopub.execute_input":"2025-04-08T12:06:43.522296Z","iopub.status.idle":"2025-04-08T16:04:19.546047Z","shell.execute_reply.started":"2025-04-08T12:06:43.522275Z","shell.execute_reply":"2025-04-08T16:04:19.545106Z"}},"outputs":[{"name":"stdout","text":"Num of trainable parameters: 21589776\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1744114106.768821     119 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"124/124 [==============================] - 464s 3s/step - seq_loss: 340.3939 - seq_acc: 0.1103 - val_seq_loss: 261.8176 - val_seq_acc: 0.2231\nEpoch 2/100\n124/124 [==============================] - 425s 3s/step - seq_loss: 237.7542 - seq_acc: 0.2361 - val_seq_loss: 195.3975 - val_seq_acc: 0.2700\nEpoch 3/100\n124/124 [==============================] - 427s 3s/step - seq_loss: 192.3358 - seq_acc: 0.2813 - val_seq_loss: 178.9964 - val_seq_acc: 0.2982\nEpoch 4/100\n124/124 [==============================] - 428s 3s/step - seq_loss: 176.7432 - seq_acc: 0.3052 - val_seq_loss: 168.7154 - val_seq_acc: 0.3081\nEpoch 5/100\n124/124 [==============================] - 426s 3s/step - seq_loss: 170.3367 - seq_acc: 0.3088 - val_seq_loss: 161.9347 - val_seq_acc: 0.3212\nEpoch 6/100\n124/124 [==============================] - 427s 3s/step - seq_loss: 161.9337 - seq_acc: 0.3260 - val_seq_loss: 156.7397 - val_seq_acc: 0.3285\nEpoch 7/100\n124/124 [==============================] - 427s 3s/step - seq_loss: 157.6452 - seq_acc: 0.3286 - val_seq_loss: 153.9445 - val_seq_acc: 0.3337\nEpoch 8/100\n124/124 [==============================] - 427s 3s/step - seq_loss: 154.0041 - seq_acc: 0.3369 - val_seq_loss: 150.3513 - val_seq_acc: 0.3379\nEpoch 9/100\n124/124 [==============================] - 427s 3s/step - seq_loss: 150.5134 - seq_acc: 0.3416 - val_seq_loss: 147.4954 - val_seq_acc: 0.3448\nEpoch 10/100\n124/124 [==============================] - 425s 3s/step - seq_loss: 148.0364 - seq_acc: 0.3451 - val_seq_loss: 142.7376 - val_seq_acc: 0.3517\nEpoch 11/100\n124/124 [==============================] - 427s 3s/step - seq_loss: 145.2245 - seq_acc: 0.3508 - val_seq_loss: 144.6599 - val_seq_acc: 0.3476\nEpoch 12/100\n124/124 [==============================] - 429s 3s/step - seq_loss: 141.8449 - seq_acc: 0.3555 - val_seq_loss: 142.6097 - val_seq_acc: 0.3455\nEpoch 13/100\n124/124 [==============================] - 427s 3s/step - seq_loss: 141.6202 - seq_acc: 0.3531 - val_seq_loss: 137.5455 - val_seq_acc: 0.3601\nEpoch 14/100\n124/124 [==============================] - 428s 3s/step - seq_loss: 140.4280 - seq_acc: 0.3566 - val_seq_loss: 135.2302 - val_seq_acc: 0.3648\nEpoch 15/100\n124/124 [==============================] - 426s 3s/step - seq_loss: 139.5243 - seq_acc: 0.3550 - val_seq_loss: 131.7864 - val_seq_acc: 0.3719\nEpoch 16/100\n124/124 [==============================] - 424s 3s/step - seq_loss: 137.4506 - seq_acc: 0.3581 - val_seq_loss: 132.8292 - val_seq_acc: 0.3665\nEpoch 17/100\n124/124 [==============================] - 423s 3s/step - seq_loss: 135.2857 - seq_acc: 0.3637 - val_seq_loss: 130.5090 - val_seq_acc: 0.3744\nEpoch 18/100\n124/124 [==============================] - 423s 3s/step - seq_loss: 133.5230 - seq_acc: 0.3652 - val_seq_loss: 130.8766 - val_seq_acc: 0.3668\nEpoch 19/100\n124/124 [==============================] - 420s 3s/step - seq_loss: 133.5309 - seq_acc: 0.3650 - val_seq_loss: 130.9288 - val_seq_acc: 0.3706\nEpoch 20/100\n124/124 [==============================] - 417s 3s/step - seq_loss: 134.7395 - seq_acc: 0.3632 - val_seq_loss: 131.4276 - val_seq_acc: 0.3623\nEpoch 21/100\n124/124 [==============================] - 417s 3s/step - seq_loss: 135.0078 - seq_acc: 0.3652 - val_seq_loss: 129.3606 - val_seq_acc: 0.3740\nEpoch 22/100\n124/124 [==============================] - 418s 3s/step - seq_loss: 136.8856 - seq_acc: 0.3552 - val_seq_loss: 135.5085 - val_seq_acc: 0.3568\nEpoch 23/100\n124/124 [==============================] - 413s 3s/step - seq_loss: 141.4060 - seq_acc: 0.3429 - val_seq_loss: 133.0922 - val_seq_acc: 0.3628\nEpoch 24/100\n124/124 [==============================] - 411s 3s/step - seq_loss: 140.2904 - seq_acc: 0.3453 - val_seq_loss: 140.4835 - val_seq_acc: 0.3475\nEpoch 25/100\n124/124 [==============================] - 407s 3s/step - seq_loss: 153.1248 - seq_acc: 0.3128 - val_seq_loss: 150.2685 - val_seq_acc: 0.3133\nEpoch 26/100\n124/124 [==============================] - 406s 3s/step - seq_loss: 156.1604 - seq_acc: 0.3056 - val_seq_loss: 149.6171 - val_seq_acc: 0.3177\nEpoch 27/100\n124/124 [==============================] - 405s 3s/step - seq_loss: 159.6654 - seq_acc: 0.2987 - val_seq_loss: 159.8316 - val_seq_acc: 0.3049\nEpoch 28/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 161.6191 - seq_acc: 0.3004 - val_seq_loss: 155.1069 - val_seq_acc: 0.3101\nEpoch 29/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 176.5266 - seq_acc: 0.2659 - val_seq_loss: 179.5087 - val_seq_acc: 0.2491\nEpoch 30/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 183.2517 - seq_acc: 0.2408 - val_seq_loss: 172.0124 - val_seq_acc: 0.2594\nEpoch 31/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 176.5933 - seq_acc: 0.2587 - val_seq_loss: 176.8084 - val_seq_acc: 0.2523\nEpoch 32/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 183.6965 - seq_acc: 0.2488 - val_seq_loss: 201.6783 - val_seq_acc: 0.1933\nEpoch 33/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 201.4584 - seq_acc: 0.2010 - val_seq_loss: 209.6800 - val_seq_acc: 0.1794\nEpoch 34/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 211.6179 - seq_acc: 0.1886 - val_seq_loss: 203.6304 - val_seq_acc: 0.1924\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"acc3, loss3, cider3 = model.eval()\nprint(f\"Accuracy: {acc3}, Loss: {loss3}, CIDEr: {cider3}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-08T16:04:19.547297Z","iopub.execute_input":"2025-04-08T16:04:19.547592Z","iopub.status.idle":"2025-04-08T16:08:12.504898Z","shell.execute_reply.started":"2025-04-08T16:04:19.547569Z","shell.execute_reply":"2025-04-08T16:08:12.503951Z"}},"outputs":[{"name":"stderr","text":"Compute Score: 100%|██████████| 394/394 [03:15<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.3706372082233429, Loss: 130.92880249023438, CIDEr: 0.09581131708700172\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Factorised Dot Product Attention","metadata":{}},{"cell_type":"code","source":"model.FactorisedDotProductAttention(D_MODELS=D_MODELS, \n                              NUM_HEADS=NUM_HEADS, \n                              MAX_FRAMES=MAX_FRAMES, \n                              SPATIAL_SIZE=SPATIAL_SIZE, \n                              VOCAB_SIZE=VOCAB_SIZE, \n                              SEQ_LENGTH=SEQ_LENGTH,\n                            NUM_CAPTIONS=NUM_CAPTIONS,\n                              NUM_L=NUM_LAYER)\n\nhistory4 = model.fit(CAPTIONS_PATH=CAPTIONS_PATH, \n                  VIDEOS_PATH=VIDEOS_PATH, \n                  FRAMES_STORAGE_PATH=FRAMES_STORAGE_PATH, \n                  EPOCHS=EPOCHS, \n                  BATCH_SIZE=BATCH_SIZE, \n                  test_size=0.2)\nmodel4 = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T08:05:42.112548Z","iopub.execute_input":"2025-04-05T08:05:42.113178Z","iopub.status.idle":"2025-04-05T18:56:15.773075Z","shell.execute_reply.started":"2025-04-05T08:05:42.113144Z","shell.execute_reply":"2025-04-05T18:56:15.772072Z"}},"outputs":[{"name":"stdout","text":"Num of trainable parameters: 19224336\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1743840451.059797     116 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"124/124 [==============================] - 436s 3s/step - seq_loss: 360.6637 - seq_acc: 0.0352 - val_seq_loss: 274.1430 - val_seq_acc: 0.1445\nEpoch 2/100\n124/124 [==============================] - 394s 3s/step - seq_loss: 244.3256 - seq_acc: 0.2034 - val_seq_loss: 198.8234 - val_seq_acc: 0.2661\nEpoch 3/100\n124/124 [==============================] - 397s 3s/step - seq_loss: 190.5506 - seq_acc: 0.2848 - val_seq_loss: 177.9953 - val_seq_acc: 0.3067\nEpoch 4/100\n124/124 [==============================] - 397s 3s/step - seq_loss: 173.2376 - seq_acc: 0.3124 - val_seq_loss: 163.2074 - val_seq_acc: 0.3250\nEpoch 5/100\n124/124 [==============================] - 398s 3s/step - seq_loss: 160.8074 - seq_acc: 0.3320 - val_seq_loss: 153.1952 - val_seq_acc: 0.3334\nEpoch 6/100\n124/124 [==============================] - 396s 3s/step - seq_loss: 150.7058 - seq_acc: 0.3422 - val_seq_loss: 145.9964 - val_seq_acc: 0.3494\nEpoch 7/100\n124/124 [==============================] - 393s 3s/step - seq_loss: 144.9254 - seq_acc: 0.3523 - val_seq_loss: 139.6167 - val_seq_acc: 0.3647\nEpoch 8/100\n124/124 [==============================] - 391s 3s/step - seq_loss: 138.2576 - seq_acc: 0.3603 - val_seq_loss: 134.3125 - val_seq_acc: 0.3660\nEpoch 9/100\n124/124 [==============================] - 396s 3s/step - seq_loss: 133.7397 - seq_acc: 0.3655 - val_seq_loss: 128.8906 - val_seq_acc: 0.3814\nEpoch 10/100\n124/124 [==============================] - 400s 3s/step - seq_loss: 129.4772 - seq_acc: 0.3731 - val_seq_loss: 124.9867 - val_seq_acc: 0.3874\nEpoch 11/100\n124/124 [==============================] - 400s 3s/step - seq_loss: 124.5739 - seq_acc: 0.3849 - val_seq_loss: 120.7718 - val_seq_acc: 0.3933\nEpoch 12/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 119.9796 - seq_acc: 0.3931 - val_seq_loss: 117.4196 - val_seq_acc: 0.4028\nEpoch 13/100\n124/124 [==============================] - 406s 3s/step - seq_loss: 117.9671 - seq_acc: 0.3939 - val_seq_loss: 114.5387 - val_seq_acc: 0.4086\nEpoch 14/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 115.2950 - seq_acc: 0.4036 - val_seq_loss: 110.3613 - val_seq_acc: 0.4199\nEpoch 15/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 111.2263 - seq_acc: 0.4169 - val_seq_loss: 107.7281 - val_seq_acc: 0.4268\nEpoch 16/100\n124/124 [==============================] - 404s 3s/step - seq_loss: 109.0568 - seq_acc: 0.4177 - val_seq_loss: 105.5031 - val_seq_acc: 0.4264\nEpoch 17/100\n124/124 [==============================] - 401s 3s/step - seq_loss: 107.7749 - seq_acc: 0.4214 - val_seq_loss: 103.3753 - val_seq_acc: 0.4410\nEpoch 18/100\n124/124 [==============================] - 400s 3s/step - seq_loss: 105.1551 - seq_acc: 0.4300 - val_seq_loss: 100.2110 - val_seq_acc: 0.4518\nEpoch 19/100\n124/124 [==============================] - 397s 3s/step - seq_loss: 101.2542 - seq_acc: 0.4459 - val_seq_loss: 96.9608 - val_seq_acc: 0.4591\nEpoch 20/100\n124/124 [==============================] - 391s 3s/step - seq_loss: 98.7737 - seq_acc: 0.4475 - val_seq_loss: 93.1603 - val_seq_acc: 0.4776\nEpoch 21/100\n124/124 [==============================] - 389s 3s/step - seq_loss: 95.7938 - seq_acc: 0.4635 - val_seq_loss: 91.1977 - val_seq_acc: 0.4837\nEpoch 22/100\n124/124 [==============================] - 397s 3s/step - seq_loss: 93.4321 - seq_acc: 0.4740 - val_seq_loss: 89.2601 - val_seq_acc: 0.4872\nEpoch 23/100\n124/124 [==============================] - 396s 3s/step - seq_loss: 90.5502 - seq_acc: 0.4856 - val_seq_loss: 87.0316 - val_seq_acc: 0.5005\nEpoch 24/100\n124/124 [==============================] - 398s 3s/step - seq_loss: 89.0717 - seq_acc: 0.4879 - val_seq_loss: 83.4705 - val_seq_acc: 0.5180\nEpoch 25/100\n124/124 [==============================] - 403s 3s/step - seq_loss: 87.4487 - seq_acc: 0.4980 - val_seq_loss: 81.7437 - val_seq_acc: 0.5195\nEpoch 26/100\n124/124 [==============================] - 402s 3s/step - seq_loss: 84.9897 - seq_acc: 0.5063 - val_seq_loss: 80.3478 - val_seq_acc: 0.5320\nEpoch 27/100\n124/124 [==============================] - 396s 3s/step - seq_loss: 82.8651 - seq_acc: 0.5180 - val_seq_loss: 79.6635 - val_seq_acc: 0.5296\nEpoch 28/100\n124/124 [==============================] - 396s 3s/step - seq_loss: 80.8640 - seq_acc: 0.5245 - val_seq_loss: 75.8386 - val_seq_acc: 0.5477\nEpoch 29/100\n124/124 [==============================] - 395s 3s/step - seq_loss: 79.2754 - seq_acc: 0.5319 - val_seq_loss: 76.0052 - val_seq_acc: 0.5496\nEpoch 30/100\n124/124 [==============================] - 393s 3s/step - seq_loss: 77.6027 - seq_acc: 0.5396 - val_seq_loss: 73.3161 - val_seq_acc: 0.5648\nEpoch 31/100\n124/124 [==============================] - 395s 3s/step - seq_loss: 76.0965 - seq_acc: 0.5459 - val_seq_loss: 72.5318 - val_seq_acc: 0.5668\nEpoch 32/100\n124/124 [==============================] - 395s 3s/step - seq_loss: 74.8518 - seq_acc: 0.5529 - val_seq_loss: 70.7821 - val_seq_acc: 0.5723\nEpoch 33/100\n124/124 [==============================] - 393s 3s/step - seq_loss: 73.3067 - seq_acc: 0.5598 - val_seq_loss: 67.9804 - val_seq_acc: 0.5877\nEpoch 34/100\n124/124 [==============================] - 391s 3s/step - seq_loss: 70.6129 - seq_acc: 0.5741 - val_seq_loss: 66.3633 - val_seq_acc: 0.5972\nEpoch 35/100\n124/124 [==============================] - 394s 3s/step - seq_loss: 69.3345 - seq_acc: 0.5781 - val_seq_loss: 64.7766 - val_seq_acc: 0.6027\nEpoch 36/100\n124/124 [==============================] - 394s 3s/step - seq_loss: 67.5393 - seq_acc: 0.5861 - val_seq_loss: 61.6957 - val_seq_acc: 0.6189\nEpoch 37/100\n124/124 [==============================] - 393s 3s/step - seq_loss: 65.1911 - seq_acc: 0.5985 - val_seq_loss: 60.7775 - val_seq_acc: 0.6162\nEpoch 38/100\n124/124 [==============================] - 395s 3s/step - seq_loss: 63.6872 - seq_acc: 0.6027 - val_seq_loss: 58.7993 - val_seq_acc: 0.6309\nEpoch 39/100\n124/124 [==============================] - 394s 3s/step - seq_loss: 61.6552 - seq_acc: 0.6129 - val_seq_loss: 58.2009 - val_seq_acc: 0.6344\nEpoch 40/100\n124/124 [==============================] - 392s 3s/step - seq_loss: 60.6741 - seq_acc: 0.6174 - val_seq_loss: 56.6383 - val_seq_acc: 0.6465\nEpoch 41/100\n124/124 [==============================] - 394s 3s/step - seq_loss: 58.5798 - seq_acc: 0.6307 - val_seq_loss: 54.8715 - val_seq_acc: 0.6519\nEpoch 42/100\n124/124 [==============================] - 394s 3s/step - seq_loss: 58.0445 - seq_acc: 0.6305 - val_seq_loss: 53.9690 - val_seq_acc: 0.6553\nEpoch 43/100\n124/124 [==============================] - 393s 3s/step - seq_loss: 55.8478 - seq_acc: 0.6418 - val_seq_loss: 52.6997 - val_seq_acc: 0.6596\nEpoch 44/100\n124/124 [==============================] - 391s 3s/step - seq_loss: 53.6209 - seq_acc: 0.6530 - val_seq_loss: 51.4214 - val_seq_acc: 0.6678\nEpoch 45/100\n124/124 [==============================] - 392s 3s/step - seq_loss: 53.1440 - seq_acc: 0.6530 - val_seq_loss: 49.9359 - val_seq_acc: 0.6769\nEpoch 46/100\n124/124 [==============================] - 391s 3s/step - seq_loss: 51.5759 - seq_acc: 0.6635 - val_seq_loss: 49.2112 - val_seq_acc: 0.6760\nEpoch 47/100\n124/124 [==============================] - 391s 3s/step - seq_loss: 51.1914 - seq_acc: 0.6645 - val_seq_loss: 47.4341 - val_seq_acc: 0.6916\nEpoch 48/100\n124/124 [==============================] - 396s 3s/step - seq_loss: 49.8136 - seq_acc: 0.6742 - val_seq_loss: 49.3812 - val_seq_acc: 0.6782\nEpoch 49/100\n124/124 [==============================] - 394s 3s/step - seq_loss: 49.4493 - seq_acc: 0.6766 - val_seq_loss: 45.8735 - val_seq_acc: 0.6979\nEpoch 50/100\n124/124 [==============================] - 394s 3s/step - seq_loss: 47.2706 - seq_acc: 0.6837 - val_seq_loss: 44.7304 - val_seq_acc: 0.7031\nEpoch 51/100\n124/124 [==============================] - 394s 3s/step - seq_loss: 46.0460 - seq_acc: 0.6924 - val_seq_loss: 43.8163 - val_seq_acc: 0.7102\nEpoch 52/100\n124/124 [==============================] - 394s 3s/step - seq_loss: 45.7304 - seq_acc: 0.6937 - val_seq_loss: 43.3226 - val_seq_acc: 0.7093\nEpoch 53/100\n124/124 [==============================] - 395s 3s/step - seq_loss: 45.0421 - seq_acc: 0.6974 - val_seq_loss: 43.4248 - val_seq_acc: 0.7095\nEpoch 54/100\n124/124 [==============================] - 396s 3s/step - seq_loss: 43.4664 - seq_acc: 0.7070 - val_seq_loss: 40.8462 - val_seq_acc: 0.7215\nEpoch 55/100\n124/124 [==============================] - 393s 3s/step - seq_loss: 43.3566 - seq_acc: 0.7068 - val_seq_loss: 41.2854 - val_seq_acc: 0.7242\nEpoch 56/100\n124/124 [==============================] - 391s 3s/step - seq_loss: 42.6264 - seq_acc: 0.7114 - val_seq_loss: 39.9051 - val_seq_acc: 0.7312\nEpoch 57/100\n124/124 [==============================] - 389s 3s/step - seq_loss: 41.9143 - seq_acc: 0.7132 - val_seq_loss: 39.5079 - val_seq_acc: 0.7315\nEpoch 58/100\n124/124 [==============================] - 384s 3s/step - seq_loss: 40.7216 - seq_acc: 0.7194 - val_seq_loss: 39.5054 - val_seq_acc: 0.7301\nEpoch 59/100\n124/124 [==============================] - 385s 3s/step - seq_loss: 42.5104 - seq_acc: 0.7085 - val_seq_loss: 37.9102 - val_seq_acc: 0.7343\nEpoch 60/100\n124/124 [==============================] - 383s 3s/step - seq_loss: 40.0349 - seq_acc: 0.7229 - val_seq_loss: 37.9981 - val_seq_acc: 0.7383\nEpoch 61/100\n124/124 [==============================] - 384s 3s/step - seq_loss: 39.9403 - seq_acc: 0.7251 - val_seq_loss: 36.8871 - val_seq_acc: 0.7445\nEpoch 62/100\n124/124 [==============================] - 384s 3s/step - seq_loss: 38.9487 - seq_acc: 0.7290 - val_seq_loss: 37.3029 - val_seq_acc: 0.7437\nEpoch 63/100\n124/124 [==============================] - 384s 3s/step - seq_loss: 38.3187 - seq_acc: 0.7338 - val_seq_loss: 37.4616 - val_seq_acc: 0.7407\nEpoch 64/100\n124/124 [==============================] - 386s 3s/step - seq_loss: 38.2449 - seq_acc: 0.7324 - val_seq_loss: 35.3357 - val_seq_acc: 0.7527\nEpoch 65/100\n124/124 [==============================] - 383s 3s/step - seq_loss: 37.4001 - seq_acc: 0.7378 - val_seq_loss: 35.2388 - val_seq_acc: 0.7514\nEpoch 66/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 36.8453 - seq_acc: 0.7379 - val_seq_loss: 35.8438 - val_seq_acc: 0.7493\nEpoch 67/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 36.5329 - seq_acc: 0.7421 - val_seq_loss: 35.9538 - val_seq_acc: 0.7450\nEpoch 68/100\n124/124 [==============================] - 383s 3s/step - seq_loss: 36.6881 - seq_acc: 0.7383 - val_seq_loss: 36.5478 - val_seq_acc: 0.7452\nEpoch 69/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 38.5176 - seq_acc: 0.7331 - val_seq_loss: 41.3398 - val_seq_acc: 0.7178\nEpoch 70/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 40.6260 - seq_acc: 0.7178 - val_seq_loss: 35.4271 - val_seq_acc: 0.7502\nEpoch 71/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 37.1752 - seq_acc: 0.7364 - val_seq_loss: 35.1238 - val_seq_acc: 0.7490\nEpoch 72/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 37.1022 - seq_acc: 0.7354 - val_seq_loss: 34.8123 - val_seq_acc: 0.7540\nEpoch 73/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 37.3116 - seq_acc: 0.7345 - val_seq_loss: 34.5992 - val_seq_acc: 0.7523\nEpoch 74/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 35.8990 - seq_acc: 0.7443 - val_seq_loss: 34.5197 - val_seq_acc: 0.7536\nEpoch 75/100\n124/124 [==============================] - 383s 3s/step - seq_loss: 35.5376 - seq_acc: 0.7452 - val_seq_loss: 33.6150 - val_seq_acc: 0.7576\nEpoch 76/100\n124/124 [==============================] - 379s 3s/step - seq_loss: 34.9358 - seq_acc: 0.7488 - val_seq_loss: 33.0914 - val_seq_acc: 0.7600\nEpoch 77/100\n124/124 [==============================] - 380s 3s/step - seq_loss: 35.0768 - seq_acc: 0.7484 - val_seq_loss: 31.8915 - val_seq_acc: 0.7687\nEpoch 78/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 33.9235 - seq_acc: 0.7540 - val_seq_loss: 32.4026 - val_seq_acc: 0.7657\nEpoch 79/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 33.7228 - seq_acc: 0.7541 - val_seq_loss: 31.8318 - val_seq_acc: 0.7684\nEpoch 80/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 33.2898 - seq_acc: 0.7550 - val_seq_loss: 33.0707 - val_seq_acc: 0.7584\nEpoch 81/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 33.2709 - seq_acc: 0.7554 - val_seq_loss: 32.0536 - val_seq_acc: 0.7669\nEpoch 82/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 32.7742 - seq_acc: 0.7584 - val_seq_loss: 31.3975 - val_seq_acc: 0.7693\nEpoch 83/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 33.1278 - seq_acc: 0.7554 - val_seq_loss: 32.1590 - val_seq_acc: 0.7629\nEpoch 84/100\n124/124 [==============================] - 380s 3s/step - seq_loss: 32.0597 - seq_acc: 0.7633 - val_seq_loss: 31.1540 - val_seq_acc: 0.7696\nEpoch 85/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 31.8062 - seq_acc: 0.7623 - val_seq_loss: 30.0182 - val_seq_acc: 0.7744\nEpoch 86/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 31.5225 - seq_acc: 0.7636 - val_seq_loss: 30.7894 - val_seq_acc: 0.7717\nEpoch 87/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 31.5324 - seq_acc: 0.7652 - val_seq_loss: 30.4245 - val_seq_acc: 0.7695\nEpoch 88/100\n124/124 [==============================] - 383s 3s/step - seq_loss: 30.8392 - seq_acc: 0.7675 - val_seq_loss: 29.5624 - val_seq_acc: 0.7774\nEpoch 89/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 30.4450 - seq_acc: 0.7702 - val_seq_loss: 29.9410 - val_seq_acc: 0.7741\nEpoch 90/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 30.4784 - seq_acc: 0.7702 - val_seq_loss: 29.4476 - val_seq_acc: 0.7763\nEpoch 91/100\n124/124 [==============================] - 380s 3s/step - seq_loss: 30.4491 - seq_acc: 0.7698 - val_seq_loss: 29.2803 - val_seq_acc: 0.7772\nEpoch 92/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 30.4357 - seq_acc: 0.7687 - val_seq_loss: 29.8681 - val_seq_acc: 0.7742\nEpoch 93/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 30.0686 - seq_acc: 0.7700 - val_seq_loss: 29.2228 - val_seq_acc: 0.7756\nEpoch 94/100\n124/124 [==============================] - 381s 3s/step - seq_loss: 30.0625 - seq_acc: 0.7697 - val_seq_loss: 28.8600 - val_seq_acc: 0.7791\nEpoch 95/100\n124/124 [==============================] - 380s 3s/step - seq_loss: 29.8852 - seq_acc: 0.7693 - val_seq_loss: 28.8232 - val_seq_acc: 0.7780\nEpoch 96/100\n124/124 [==============================] - 383s 3s/step - seq_loss: 29.6985 - seq_acc: 0.7706 - val_seq_loss: 29.0345 - val_seq_acc: 0.7782\nEpoch 97/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 29.2961 - seq_acc: 0.7747 - val_seq_loss: 28.5747 - val_seq_acc: 0.7792\nEpoch 98/100\n124/124 [==============================] - 382s 3s/step - seq_loss: 29.4736 - seq_acc: 0.7725 - val_seq_loss: 28.1238 - val_seq_acc: 0.7804\nEpoch 99/100\n124/124 [==============================] - 384s 3s/step - seq_loss: 29.2557 - seq_acc: 0.7722 - val_seq_loss: 28.5138 - val_seq_acc: 0.7786\nEpoch 100/100\n124/124 [==============================] - 383s 3s/step - seq_loss: 29.2582 - seq_acc: 0.7722 - val_seq_loss: 28.0313 - val_seq_acc: 0.7810\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"acc4, loss4, cider4 = model.eval()\nprint(f\"Accuracy: {acc4}, Loss: {loss4}, CIDEr: {cider4}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:56:15.774629Z","iopub.execute_input":"2025-04-05T18:56:15.774896Z","iopub.status.idle":"2025-04-05T19:00:16.077995Z","shell.execute_reply.started":"2025-04-05T18:56:15.774875Z","shell.execute_reply":"2025-04-05T19:00:16.077037Z"}},"outputs":[{"name":"stderr","text":"Compute Score: 100%|██████████| 394/394 [03:24<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.780953049659729, Loss: 28.031295776367188, CIDEr: 1.667939180012785\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Cross Attention","metadata":{}},{"cell_type":"code","source":"model.CrossAttention(D_MODELS=D_MODELS, \n                  NUM_HEADS=NUM_HEADS, \n                  MAX_FRAMES=MAX_FRAMES, \n                  SPATIAL_SIZE=SPATIAL_SIZE, \n                  VOCAB_SIZE=VOCAB_SIZE, \n                  SEQ_LENGTH=SEQ_LENGTH,\n                NUM_CAPTIONS=NUM_CAPTIONS,\n                  NUM_L=NUM_LAYER)\n\nhistory5 = model.fit(CAPTIONS_PATH=CAPTIONS_PATH, \n                  VIDEOS_PATH=VIDEOS_PATH, \n                  FRAMES_STORAGE_PATH=FRAMES_STORAGE_PATH, \n                  EPOCHS=EPOCHS, \n                  BATCH_SIZE=BATCH_SIZE,  \n                  test_size=0.2)\nmodel5 = model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T08:55:51.305500Z","iopub.execute_input":"2025-03-30T08:55:51.306199Z","iopub.status.idle":"2025-03-30T15:04:11.657441Z","shell.execute_reply.started":"2025-03-30T08:55:51.306167Z","shell.execute_reply":"2025-03-30T15:04:11.656532Z"}},"outputs":[{"name":"stdout","text":"Num of trainable parameters: 26760362\n","output_type":"stream"},{"name":"stderr","text":"Saving frames: 100%|██████████| 1970/1970 [10:35<00:00,  3.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1743325693.941374     130 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"124/124 [==============================] - 271s 1s/step - seq_loss: 359.4393 - seq_acc: 0.0453 - val_seq_loss: 270.3019 - val_seq_acc: 0.1456\nEpoch 2/100\n124/124 [==============================] - 217s 1s/step - seq_loss: 243.0401 - seq_acc: 0.2147 - val_seq_loss: 198.3619 - val_seq_acc: 0.2710\nEpoch 3/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 192.3208 - seq_acc: 0.2837 - val_seq_loss: 176.6787 - val_seq_acc: 0.3081\nEpoch 4/100\n124/124 [==============================] - 216s 1s/step - seq_loss: 174.2061 - seq_acc: 0.3140 - val_seq_loss: 161.9445 - val_seq_acc: 0.3295\nEpoch 5/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 162.6961 - seq_acc: 0.3243 - val_seq_loss: 151.9797 - val_seq_acc: 0.3354\nEpoch 6/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 151.8924 - seq_acc: 0.3428 - val_seq_loss: 144.0253 - val_seq_acc: 0.3560\nEpoch 7/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 145.8852 - seq_acc: 0.3485 - val_seq_loss: 138.1168 - val_seq_acc: 0.3623\nEpoch 8/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 138.7419 - seq_acc: 0.3614 - val_seq_loss: 132.0987 - val_seq_acc: 0.3756\nEpoch 9/100\n124/124 [==============================] - 216s 1s/step - seq_loss: 134.6032 - seq_acc: 0.3667 - val_seq_loss: 127.8913 - val_seq_acc: 0.3721\nEpoch 10/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 129.5737 - seq_acc: 0.3767 - val_seq_loss: 123.0429 - val_seq_acc: 0.3929\nEpoch 11/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 125.2015 - seq_acc: 0.3832 - val_seq_loss: 119.3799 - val_seq_acc: 0.4022\nEpoch 12/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 122.4816 - seq_acc: 0.3849 - val_seq_loss: 115.0701 - val_seq_acc: 0.4021\nEpoch 13/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 118.4832 - seq_acc: 0.3959 - val_seq_loss: 113.3557 - val_seq_acc: 0.4078\nEpoch 14/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 116.4188 - seq_acc: 0.3992 - val_seq_loss: 109.3468 - val_seq_acc: 0.4249\nEpoch 15/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 112.3794 - seq_acc: 0.4111 - val_seq_loss: 106.7344 - val_seq_acc: 0.4287\nEpoch 16/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 110.8796 - seq_acc: 0.4129 - val_seq_loss: 103.6760 - val_seq_acc: 0.4388\nEpoch 17/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 106.2996 - seq_acc: 0.4272 - val_seq_loss: 100.7501 - val_seq_acc: 0.4465\nEpoch 18/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 105.3063 - seq_acc: 0.4316 - val_seq_loss: 98.0672 - val_seq_acc: 0.4597\nEpoch 19/100\n124/124 [==============================] - 216s 1s/step - seq_loss: 102.0950 - seq_acc: 0.4440 - val_seq_loss: 94.3732 - val_seq_acc: 0.4736\nEpoch 20/100\n124/124 [==============================] - 216s 1s/step - seq_loss: 99.4896 - seq_acc: 0.4497 - val_seq_loss: 92.5962 - val_seq_acc: 0.4806\nEpoch 21/100\n124/124 [==============================] - 216s 1s/step - seq_loss: 96.8602 - seq_acc: 0.4601 - val_seq_loss: 91.9287 - val_seq_acc: 0.4720\nEpoch 22/100\n124/124 [==============================] - 216s 1s/step - seq_loss: 96.1928 - seq_acc: 0.4611 - val_seq_loss: 89.3887 - val_seq_acc: 0.4927\nEpoch 23/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 93.9845 - seq_acc: 0.4708 - val_seq_loss: 88.1524 - val_seq_acc: 0.4962\nEpoch 24/100\n124/124 [==============================] - 216s 1s/step - seq_loss: 90.8986 - seq_acc: 0.4870 - val_seq_loss: 85.2230 - val_seq_acc: 0.5100\nEpoch 25/100\n124/124 [==============================] - 217s 1s/step - seq_loss: 89.9330 - seq_acc: 0.4877 - val_seq_loss: 83.0558 - val_seq_acc: 0.5206\nEpoch 26/100\n124/124 [==============================] - 216s 1s/step - seq_loss: 87.6593 - seq_acc: 0.4952 - val_seq_loss: 81.9616 - val_seq_acc: 0.5285\nEpoch 27/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 85.5371 - seq_acc: 0.5046 - val_seq_loss: 80.1986 - val_seq_acc: 0.5383\nEpoch 28/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 84.6715 - seq_acc: 0.5097 - val_seq_loss: 78.2726 - val_seq_acc: 0.5411\nEpoch 29/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 82.6028 - seq_acc: 0.5182 - val_seq_loss: 76.4290 - val_seq_acc: 0.5524\nEpoch 30/100\n124/124 [==============================] - 216s 1s/step - seq_loss: 81.3521 - seq_acc: 0.5279 - val_seq_loss: 77.8153 - val_seq_acc: 0.5403\nEpoch 31/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 81.3954 - seq_acc: 0.5282 - val_seq_loss: 74.1937 - val_seq_acc: 0.5627\nEpoch 32/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 78.7225 - seq_acc: 0.5384 - val_seq_loss: 72.4660 - val_seq_acc: 0.5706\nEpoch 33/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 76.6790 - seq_acc: 0.5481 - val_seq_loss: 73.4432 - val_seq_acc: 0.5698\nEpoch 34/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 76.0423 - seq_acc: 0.5533 - val_seq_loss: 70.0517 - val_seq_acc: 0.5828\nEpoch 35/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 74.5045 - seq_acc: 0.5578 - val_seq_loss: 68.5167 - val_seq_acc: 0.5919\nEpoch 36/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 72.9753 - seq_acc: 0.5669 - val_seq_loss: 67.1245 - val_seq_acc: 0.5981\nEpoch 37/100\n124/124 [==============================] - 216s 1s/step - seq_loss: 71.6954 - seq_acc: 0.5699 - val_seq_loss: 67.9660 - val_seq_acc: 0.5943\nEpoch 38/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 70.2649 - seq_acc: 0.5806 - val_seq_loss: 64.4416 - val_seq_acc: 0.6083\nEpoch 39/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 68.3776 - seq_acc: 0.5898 - val_seq_loss: 62.5198 - val_seq_acc: 0.6187\nEpoch 40/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 66.9459 - seq_acc: 0.5964 - val_seq_loss: 62.0440 - val_seq_acc: 0.6184\nEpoch 41/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 64.6941 - seq_acc: 0.6038 - val_seq_loss: 60.3481 - val_seq_acc: 0.6307\nEpoch 42/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 63.7936 - seq_acc: 0.6103 - val_seq_loss: 58.6227 - val_seq_acc: 0.6384\nEpoch 43/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 62.9896 - seq_acc: 0.6116 - val_seq_loss: 58.4619 - val_seq_acc: 0.6366\nEpoch 44/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 62.4911 - seq_acc: 0.6159 - val_seq_loss: 56.6134 - val_seq_acc: 0.6479\nEpoch 45/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 59.6622 - seq_acc: 0.6267 - val_seq_loss: 56.4011 - val_seq_acc: 0.6505\nEpoch 46/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 60.9231 - seq_acc: 0.6200 - val_seq_loss: 55.2829 - val_seq_acc: 0.6554\nEpoch 47/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 58.8474 - seq_acc: 0.6302 - val_seq_loss: 55.2366 - val_seq_acc: 0.6502\nEpoch 48/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 57.6764 - seq_acc: 0.6379 - val_seq_loss: 52.1730 - val_seq_acc: 0.6671\nEpoch 49/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 56.1684 - seq_acc: 0.6413 - val_seq_loss: 52.4535 - val_seq_acc: 0.6649\nEpoch 50/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 56.0037 - seq_acc: 0.6460 - val_seq_loss: 53.6925 - val_seq_acc: 0.6607\nEpoch 51/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 56.3765 - seq_acc: 0.6445 - val_seq_loss: 50.2078 - val_seq_acc: 0.6794\nEpoch 52/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 53.9932 - seq_acc: 0.6527 - val_seq_loss: 50.7477 - val_seq_acc: 0.6761\nEpoch 53/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 53.0480 - seq_acc: 0.6597 - val_seq_loss: 49.2097 - val_seq_acc: 0.6822\nEpoch 54/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 53.0819 - seq_acc: 0.6567 - val_seq_loss: 48.3132 - val_seq_acc: 0.6848\nEpoch 55/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 52.1811 - seq_acc: 0.6631 - val_seq_loss: 48.3613 - val_seq_acc: 0.6850\nEpoch 56/100\n124/124 [==============================] - 215s 1s/step - seq_loss: 51.1752 - seq_acc: 0.6655 - val_seq_loss: 47.4810 - val_seq_acc: 0.6911\nEpoch 57/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 50.0150 - seq_acc: 0.6749 - val_seq_loss: 46.2286 - val_seq_acc: 0.6975\nEpoch 58/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 49.5235 - seq_acc: 0.6745 - val_seq_loss: 44.9987 - val_seq_acc: 0.7012\nEpoch 59/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 47.9051 - seq_acc: 0.6843 - val_seq_loss: 45.1717 - val_seq_acc: 0.6993\nEpoch 60/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 47.6783 - seq_acc: 0.6824 - val_seq_loss: 44.7524 - val_seq_acc: 0.7014\nEpoch 61/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 46.5929 - seq_acc: 0.6900 - val_seq_loss: 43.5968 - val_seq_acc: 0.7066\nEpoch 62/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 45.7196 - seq_acc: 0.6957 - val_seq_loss: 42.5407 - val_seq_acc: 0.7118\nEpoch 63/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 45.3490 - seq_acc: 0.6971 - val_seq_loss: 41.7719 - val_seq_acc: 0.7160\nEpoch 64/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 45.0376 - seq_acc: 0.6954 - val_seq_loss: 41.9023 - val_seq_acc: 0.7166\nEpoch 65/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 44.5332 - seq_acc: 0.6983 - val_seq_loss: 40.9107 - val_seq_acc: 0.7232\nEpoch 66/100\n124/124 [==============================] - 211s 1s/step - seq_loss: 43.3576 - seq_acc: 0.7066 - val_seq_loss: 40.7104 - val_seq_acc: 0.7234\nEpoch 67/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 42.9675 - seq_acc: 0.7068 - val_seq_loss: 39.6565 - val_seq_acc: 0.7270\nEpoch 68/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 42.1893 - seq_acc: 0.7106 - val_seq_loss: 39.6944 - val_seq_acc: 0.7286\nEpoch 69/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 42.4347 - seq_acc: 0.7112 - val_seq_loss: 38.8259 - val_seq_acc: 0.7309\nEpoch 70/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 41.2819 - seq_acc: 0.7167 - val_seq_loss: 38.0279 - val_seq_acc: 0.7376\nEpoch 71/100\n124/124 [==============================] - 211s 1s/step - seq_loss: 41.0887 - seq_acc: 0.7148 - val_seq_loss: 37.4295 - val_seq_acc: 0.7402\nEpoch 72/100\n124/124 [==============================] - 211s 1s/step - seq_loss: 40.0155 - seq_acc: 0.7226 - val_seq_loss: 38.1215 - val_seq_acc: 0.7383\nEpoch 73/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 39.7601 - seq_acc: 0.7250 - val_seq_loss: 36.8778 - val_seq_acc: 0.7438\nEpoch 74/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 39.1795 - seq_acc: 0.7272 - val_seq_loss: 36.7137 - val_seq_acc: 0.7405\nEpoch 75/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 39.1581 - seq_acc: 0.7281 - val_seq_loss: 36.1959 - val_seq_acc: 0.7466\nEpoch 76/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 38.3456 - seq_acc: 0.7316 - val_seq_loss: 35.2917 - val_seq_acc: 0.7515\nEpoch 77/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 37.9195 - seq_acc: 0.7337 - val_seq_loss: 35.3912 - val_seq_acc: 0.7516\nEpoch 78/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 38.1557 - seq_acc: 0.7294 - val_seq_loss: 34.9411 - val_seq_acc: 0.7518\nEpoch 79/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 37.0660 - seq_acc: 0.7392 - val_seq_loss: 34.8591 - val_seq_acc: 0.7511\nEpoch 80/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 36.9107 - seq_acc: 0.7392 - val_seq_loss: 34.2309 - val_seq_acc: 0.7548\nEpoch 81/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 36.3891 - seq_acc: 0.7399 - val_seq_loss: 34.6181 - val_seq_acc: 0.7542\nEpoch 82/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 36.1906 - seq_acc: 0.7412 - val_seq_loss: 33.9021 - val_seq_acc: 0.7562\nEpoch 83/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 35.9108 - seq_acc: 0.7440 - val_seq_loss: 33.3946 - val_seq_acc: 0.7604\nEpoch 84/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 35.3267 - seq_acc: 0.7471 - val_seq_loss: 33.3090 - val_seq_acc: 0.7574\nEpoch 85/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 35.1449 - seq_acc: 0.7490 - val_seq_loss: 32.7634 - val_seq_acc: 0.7643\nEpoch 86/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 34.4839 - seq_acc: 0.7528 - val_seq_loss: 32.3996 - val_seq_acc: 0.7633\nEpoch 87/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 34.3351 - seq_acc: 0.7522 - val_seq_loss: 32.1178 - val_seq_acc: 0.7651\nEpoch 88/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 33.8952 - seq_acc: 0.7554 - val_seq_loss: 32.0395 - val_seq_acc: 0.7666\nEpoch 89/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 34.3757 - seq_acc: 0.7510 - val_seq_loss: 32.1802 - val_seq_acc: 0.7659\nEpoch 90/100\n124/124 [==============================] - 214s 1s/step - seq_loss: 33.7538 - seq_acc: 0.7543 - val_seq_loss: 32.0787 - val_seq_acc: 0.7658\nEpoch 91/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 33.7909 - seq_acc: 0.7537 - val_seq_loss: 31.9370 - val_seq_acc: 0.7665\nEpoch 92/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 33.3244 - seq_acc: 0.7554 - val_seq_loss: 30.9693 - val_seq_acc: 0.7702\nEpoch 93/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 32.1706 - seq_acc: 0.7628 - val_seq_loss: 30.5763 - val_seq_acc: 0.7701\nEpoch 94/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 32.4968 - seq_acc: 0.7603 - val_seq_loss: 30.6056 - val_seq_acc: 0.7711\nEpoch 95/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 32.6442 - seq_acc: 0.7598 - val_seq_loss: 30.5993 - val_seq_acc: 0.7700\nEpoch 96/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 32.0071 - seq_acc: 0.7625 - val_seq_loss: 30.4342 - val_seq_acc: 0.7705\nEpoch 97/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 31.7409 - seq_acc: 0.7638 - val_seq_loss: 29.9151 - val_seq_acc: 0.7737\nEpoch 98/100\n124/124 [==============================] - 213s 1s/step - seq_loss: 32.2435 - seq_acc: 0.7607 - val_seq_loss: 30.0183 - val_seq_acc: 0.7745\nEpoch 99/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 31.4527 - seq_acc: 0.7655 - val_seq_loss: 30.0893 - val_seq_acc: 0.7729\nEpoch 100/100\n124/124 [==============================] - 212s 1s/step - seq_loss: 31.7309 - seq_acc: 0.7629 - val_seq_loss: 29.6839 - val_seq_acc: 0.7740\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"acc5, loss5, cider5 = model.eval()\nprint(f\"Accuracy: {acc5}, Loss: {loss5}, CIDEr: {cider5}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T15:04:11.659500Z","iopub.execute_input":"2025-03-30T15:04:11.660173Z","iopub.status.idle":"2025-03-30T15:08:12.669218Z","shell.execute_reply.started":"2025-03-30T15:04:11.660141Z","shell.execute_reply":"2025-03-30T15:08:12.668328Z"}},"outputs":[{"name":"stderr","text":"Compute Score: 100%|██████████| 394/394 [03:34<00:00,  1.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.7740356922149658, Loss: 29.683887481689453, CIDEr: 1.601302119227379\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Compare","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\nmodels = [\"SpaTem\", \"FactEnc\", \"FactSAtt\", \"FactDPro\", \"CAtt\"]\nacc = [acc1, acc2, acc3, acc4, acc5]\nloss = [loss1, loss2, loss3, loss4, loss5]\ncider = [cider1, cider2, cider3, cider4, cider5]\n\nmetrics = [acc, loss, cider]\nmetric_names = [\"Accuracy\", \"Loss\", \"CIDEr\"]\ncolors = [\"b\", \"r\", \"g\"]\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten() \n\nfor i in range(3):\n    axes[i].bar(models, metrics[i], color=colors[i]) \n    axes[i].set_title(f\"Comparison of {metric_names[i]}\")\n    axes[i].set_ylabel(\"Values\")\n    axes[i].set_xlabel(\"Models\")\n    axes[i].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport numpy as np\n\ndef get_runtime_ms(model, tensor, tensor2, num_trials=100):\n    @tf.function\n    def infer():\n        en_out = model.model.encoder(tensor)\n        return model.model.decoder([tensor2, en_out, tensor2])\n\n    for _ in range(10):\n        _ = infer()\n\n    times = []\n    \n    for _ in range(num_trials):\n        start = time.perf_counter()\n        _ = infer()\n        end = time.perf_counter()\n        times.append((end - start) * 1000)\n    \n    return np.mean(times) \n\ntensor = tf.random.normal((1, MAX_FRAMES, SPATIAL_SIZE, SPATIAL_SIZE, 3))\ntensor2 = tf.random.normal((1, SEQ_LENGTH-1))\n\ndef get_flops(model, inputs) -> float:\n    if not isinstance(model, (tf.keras.models.Sequential, tf.keras.models.Model)):\n        raise ValueError(\"Only `tf.keras.Model` or `tf.keras.Sequential` instances are supported.\")\n\n    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2_as_graph\n    real_model = tf.function(model).get_concrete_function(inputs)\n    frozen_func, _ = convert_variables_to_constants_v2_as_graph(real_model)\n    run_meta = tf.compat.v1.RunMetadata()\n    opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n    flops = tf.compat.v1.profiler.profile(\n        graph=frozen_func.graph, run_meta=run_meta, cmd=\"scope\", options=opts\n    )\n    return flops.total_float_ops / 1e9\n\ncheck_flops = CreateModel(seed=True, multigpu=True)\ncheck_flops.SpatioTemporalAttention(D_MODELS=D_MODELS, \n                          NUM_HEADS=NUM_HEADS, \n                          MAX_FRAMES=MAX_FRAMES, \n                          SPATIAL_SIZE=SPATIAL_SIZE, \n                          VOCAB_SIZE=VOCAB_SIZE, \n                          SEQ_LENGTH=SEQ_LENGTH, \n                          NUM_CAPTIONS=NUM_CAPTIONS,\n                          NUM_L=NUM_BLOCKS)\nms1 = get_runtime_ms(check_flops, tensor, tensor2)\n\nshape = (1, MAX_FRAMES, SPATIAL_SIZE, SPATIAL_SIZE, 3)\ndummy_input = tf.random.normal(shape)\ncheck_flops.model.encoder(dummy_input)\nflops = get_flops(check_flops.model.encoder, tf.TensorSpec(shape, tf.float32))\n\nshape1 = (1, SEQ_LENGTH-1)\nshape2 = (1, int((MAX_FRAMES*SPATIAL_SIZE**2)/(2*16*16)), 512)\nshape3 = (1, SEQ_LENGTH-1)\ndummy_input1 = tf.random.normal(shape1)\ndummy_input2 = tf.random.normal(shape2)\ndummy_input3 = tf.random.normal(shape3)\ncheck_flops.model.decoder((dummy_input1, dummy_input2, dummy_input3))\n\nflops_ = get_flops(check_flops.model.decoder, \n                   (tf.TensorSpec(shape1, tf.float32),\n                   tf.TensorSpec(shape2, tf.float32),\n                   tf.TensorSpec(shape3, tf.float32)))\nflops1 = flops+flops_\n\ncheck_flops.FactorisedEncoder(D_MODELS=D_MODELS, \n                              NUM_HEADS=NUM_HEADS, \n                              MAX_FRAMES=MAX_FRAMES, \n                              SPATIAL_SIZE=SPATIAL_SIZE, \n                              VOCAB_SIZE=VOCAB_SIZE, \n                              SEQ_LENGTH=SEQ_LENGTH, \n                              NUM_CAPTIONS=NUM_CAPTIONS,\n                              NUM_L=NUM_BLOCKS)\nms2 = get_runtime_ms(check_flops, tensor, tensor2)\n\nshape = (1, MAX_FRAMES, SPATIAL_SIZE, SPATIAL_SIZE, 3)\ndummy_input = tf.random.normal(shape)\ncheck_flops.model.encoder(dummy_input)\nflops = get_flops(check_flops.model.encoder, tf.TensorSpec(shape, tf.float32))\n\nshape1 = (1, SEQ_LENGTH-1)\nshape2 = (1, MAX_FRAMES//2, 512)\nshape3 = (1, SEQ_LENGTH-1)\ndummy_input1 = tf.random.normal(shape1)\ndummy_input2 = tf.random.normal(shape2)\ndummy_input3 = tf.random.normal(shape3)\ncheck_flops.model.decoder((dummy_input1, dummy_input2, dummy_input3))\nflops_ = get_flops(check_flops.model.decoder, \n                   (tf.TensorSpec(shape1, tf.float32),\n                   tf.TensorSpec(shape2, tf.float32),\n                   tf.TensorSpec(shape3, tf.float32)))\nflops2 = flops+flops_\n\ncheck_flops.FactorisedSelfAttention(D_MODELS=D_MODELS, \n                              NUM_HEADS=NUM_HEADS, \n                              MAX_FRAMES=MAX_FRAMES, \n                              SPATIAL_SIZE=SPATIAL_SIZE, \n                              VOCAB_SIZE=VOCAB_SIZE, \n                              SEQ_LENGTH=SEQ_LENGTH, \n                              NUM_CAPTIONS=NUM_CAPTIONS,\n                              NUM_L=NUM_BLOCKS)\nms3 = get_runtime_ms(check_flops, tensor, tensor2)\n\nshape = (1, MAX_FRAMES, SPATIAL_SIZE, SPATIAL_SIZE, 3)\ndummy_input = tf.random.normal(shape)\ncheck_flops.model.encoder(dummy_input)\nflops = get_flops(check_flops.model.encoder, tf.TensorSpec(shape, tf.float32))\n\nshape1 = (1, SEQ_LENGTH-1)\nshape2 = (1, int((MAX_FRAMES*SPATIAL_SIZE**2)/(2*16*16)), 512)\nshape3 = (1, SEQ_LENGTH-1)\ndummy_input1 = tf.random.normal(shape1)\ndummy_input2 = tf.random.normal(shape2)\ndummy_input3 = tf.random.normal(shape3)\ncheck_flops.model.decoder((dummy_input1, dummy_input2, dummy_input3))\nflops_ = get_flops(check_flops.model.decoder, \n                   (tf.TensorSpec(shape1, tf.float32),\n                   tf.TensorSpec(shape2, tf.float32),\n                   tf.TensorSpec(shape3, tf.float32)))\nflops3 = flops+flops_\n\ncheck_flops.FactorisedDotProductAttention(D_MODELS=D_MODELS, \n                              NUM_HEADS=NUM_HEADS, \n                              MAX_FRAMES=MAX_FRAMES, \n                              SPATIAL_SIZE=SPATIAL_SIZE, \n                              VOCAB_SIZE=VOCAB_SIZE, \n                              SEQ_LENGTH=SEQ_LENGTH, \n                              NUM_CAPTIONS=NUM_CAPTIONS,\n                              NUM_L=NUM_BLOCKS)\nms4 = get_runtime_ms(check_flops, tensor, tensor2)\n\nshape = (1, MAX_FRAMES, SPATIAL_SIZE, SPATIAL_SIZE, 3)\ndummy_input = tf.random.normal(shape)\ncheck_flops.model.encoder(dummy_input)\nflops = get_flops(check_flops.model.encoder, tf.TensorSpec(shape, tf.float32))\n\nshape1 = (1, SEQ_LENGTH-1)\nshape2 = (1, int((MAX_FRAMES*SPATIAL_SIZE**2)/(2*16*16)), 512)\nshape3 = (1, SEQ_LENGTH-1)\ndummy_input1 = tf.random.normal(shape1)\ndummy_input2 = tf.random.normal(shape2)\ndummy_input3 = tf.random.normal(shape3)\ncheck_flops.model.decoder((dummy_input1, dummy_input2, dummy_input3))\nflops_ = get_flops(check_flops.model.decoder, \n                   (tf.TensorSpec(shape1, tf.float32),\n                   tf.TensorSpec(shape2, tf.float32),\n                   tf.TensorSpec(shape3, tf.float32)))\nflops4 = flops+flops_\n\ncheck_flops.CrossAttention(D_MODELS=D_MODELS, \n                              NUM_HEADS=NUM_HEADS, \n                              MAX_FRAMES=MAX_FRAMES, \n                              SPATIAL_SIZE=SPATIAL_SIZE, \n                              VOCAB_SIZE=VOCAB_SIZE, \n                              SEQ_LENGTH=SEQ_LENGTH, \n                              NUM_CAPTIONS=NUM_CAPTIONS,\n                              NUM_L=NUM_BLOCKS)\nms5 = get_runtime_ms(check_flops, tensor, tensor2)\n\nshape = (1, MAX_FRAMES, SPATIAL_SIZE, SPATIAL_SIZE, 3)\ndummy_input = tf.random.normal(shape)\ncheck_flops.model.encoder(dummy_input)\nflops = get_flops(check_flops.model.encoder, tf.TensorSpec(shape, tf.float32))\n\nshape1 = (1, SEQ_LENGTH-1)\nshape2 = (1, int((SPATIAL_SIZE/16)**2) + MAX_FRAMES//2, 512)\nshape3 = (1, SEQ_LENGTH-1)\ndummy_input1 = tf.random.normal(shape1)\ndummy_input2 = tf.random.normal(shape2)\ndummy_input3 = tf.random.normal(shape3)\ncheck_flops.model.decoder((dummy_input1, dummy_input2, dummy_input3))\nflops_ = get_flops(check_flops.model.decoder, \n                   (tf.TensorSpec(shape1, tf.float32),\n                   tf.TensorSpec(shape2, tf.float32),\n                   tf.TensorSpec(shape3, tf.float32)))\nflops5 = flops+flops_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T20:35:05.231772Z","iopub.execute_input":"2025-03-29T20:35:05.232133Z","iopub.status.idle":"2025-03-29T20:35:42.427877Z","shell.execute_reply.started":"2025-03-29T20:35:05.232107Z","shell.execute_reply":"2025-03-29T20:35:42.426269Z"}},"outputs":[{"name":"stdout","text":"Num of trainable parameters: 19486480\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/6.06b flops)\n  encoder_1/patch_embedding_1/conv3d_1/Conv3D (2.47b/2.47b flops)\n  encoder_1/encoder_block_2/dense_10/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_1/encoder_block_2/dense_11/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_1/encoder_block_3/dense_12/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_1/encoder_block_3/dense_13/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_1/encoder_block_2/multi_head_attention_5/softmax/Softmax (98.34m/98.34m flops)\n  encoder_1/encoder_block_3/multi_head_attention_6/softmax_1/Softmax (98.34m/98.34m flops)\n  encoder_1/encoder_block_2/multi_head_attention_5/dropout/dropout/GreaterEqual (19.67m/19.67m flops)\n  encoder_1/encoder_block_2/multi_head_attention_5/dropout/dropout/Mul (19.67m/19.67m flops)\n  encoder_1/encoder_block_3/multi_head_attention_6/dropout_1/dropout/GreaterEqual (19.67m/19.67m flops)\n  encoder_1/encoder_block_3/multi_head_attention_6/dropout_1/dropout/Mul (19.67m/19.67m flops)\n  encoder_1/add/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/add_1/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/add_2/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/dense_10/BiasAdd (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/dense_10/Gelu/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/dense_10/Gelu/mul (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/dense_10/Gelu/mul_1 (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/dense_10/Gelu/truediv (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/dense_11/BiasAdd (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/dropout_2/dropout/GreaterEqual (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/dropout_2/dropout/Mul (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/layer_normalization_10/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/layer_normalization_10/mul (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/layer_normalization_9/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/layer_normalization_9/mul (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/multi_head_attention_5/Mul (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/multi_head_attention_5/attention_output/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/multi_head_attention_5/key/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/multi_head_attention_5/query/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_2/multi_head_attention_5/value/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/add_3/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/add_4/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/dense_12/BiasAdd (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/dense_12/Gelu/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/dense_12/Gelu/mul (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/dense_12/Gelu/mul_1 (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/dense_12/Gelu/truediv (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/dense_13/BiasAdd (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/dropout_3/dropout/GreaterEqual (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/dropout_3/dropout/Mul (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/layer_normalization_11/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/layer_normalization_11/mul (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/layer_normalization_12/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/layer_normalization_12/mul (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/multi_head_attention_6/Mul (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/multi_head_attention_6/attention_output/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/multi_head_attention_6/key/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/multi_head_attention_6/query/add (802.82k/802.82k flops)\n  encoder_1/encoder_block_3/multi_head_attention_6/value/add (802.82k/802.82k flops)\n  encoder_1/patch_embedding_1/conv3d_1/BiasAdd (802.82k/802.82k flops)\n\n======================End of Report==========================\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/513.95m flops)\n  decoder_1/dense_19/Tensordot/MatMul (399.36m/399.36m flops)\n  decoder_1/dense_18/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_1/transformer_block_2/dense_14/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_1/transformer_block_2/dense_15/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_1/transformer_block_3/dense_16/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_1/transformer_block_3/dense_17/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/softmax_1/Softmax (2.45m/2.45m flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/softmax_2/Softmax (2.45m/2.45m flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/key/add (802.82k/802.82k flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/value/add (802.82k/802.82k flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/key/add (802.82k/802.82k flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/value/add (802.82k/802.82k flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/dropout_1/dropout/GreaterEqual (489.22k/489.22k flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/dropout_1/dropout/Mul (489.22k/489.22k flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/softmax_1/add (489.22k/489.22k flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/dropout_2/dropout/GreaterEqual (489.22k/489.22k flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/dropout_2/dropout/Mul (489.22k/489.22k flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/softmax_2/add (489.22k/489.22k flops)\n  decoder_1/dense_19/BiasAdd (390.00k/390.00k flops)\n  decoder_1/multi_head_attention_7/softmax/Softmax (60.84k/60.84k flops)\n  decoder_1/add/add (19.97k/19.97k flops)\n  decoder_1/add_1/add (19.97k/19.97k flops)\n  decoder_1/dense_18/BiasAdd (19.97k/19.97k flops)\n  decoder_1/layer_normalization_13/add (19.97k/19.97k flops)\n  decoder_1/layer_normalization_13/mul (19.97k/19.97k flops)\n  decoder_1/mul (19.97k/19.97k flops)\n  decoder_1/multi_head_attention_7/Mul (19.97k/19.97k flops)\n  decoder_1/multi_head_attention_7/attention_output/add (19.97k/19.97k flops)\n  decoder_1/multi_head_attention_7/key/add (19.97k/19.97k flops)\n  decoder_1/multi_head_attention_7/query/add (19.97k/19.97k flops)\n  decoder_1/multi_head_attention_7/value/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/add_2/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/add_3/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/dense_14/BiasAdd (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/dense_14/Gelu/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/dense_14/Gelu/mul (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/dense_14/Gelu/mul_1 (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/dense_14/Gelu/truediv (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/dense_15/BiasAdd (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/layer_normalization_14/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/layer_normalization_14/mul (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/layer_normalization_15/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/layer_normalization_15/mul (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/Mul (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/attention_output/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/query/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/add_4/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/add_5/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/dense_16/BiasAdd (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/dense_16/Gelu/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/dense_16/Gelu/mul (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/dense_16/Gelu/mul_1 (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/dense_16/Gelu/truediv (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/dense_17/BiasAdd (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/layer_normalization_16/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/layer_normalization_16/mul (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/layer_normalization_17/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/layer_normalization_17/mul (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/Mul (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/attention_output/add (19.97k/19.97k flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/query/add (19.97k/19.97k flops)\n  decoder_1/multi_head_attention_7/dropout/dropout/GreaterEqual (12.17k/12.17k flops)\n  decoder_1/multi_head_attention_7/dropout/dropout/Mul (12.17k/12.17k flops)\n  decoder_1/multi_head_attention_7/softmax/add (12.17k/12.17k flops)\n  decoder_1/GreaterEqual (1.52k/1.52k flops)\n  decoder_1/Minimum (1.52k/1.52k flops)\n  decoder_1/multi_head_attention_7/softmax/mul (1.52k/1.52k flops)\n  decoder_1/multi_head_attention_7/softmax/sub (1.52k/1.52k flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/softmax_1/mul (39/39 flops)\n  decoder_1/transformer_block_2/multi_head_attention_8/softmax_1/sub (39/39 flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/softmax_2/mul (39/39 flops)\n  decoder_1/transformer_block_3/multi_head_attention_9/softmax_2/sub (39/39 flops)\n\n======================End of Report==========================\nNum of trainable parameters: 22642448\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/5.84b flops)\n  encoder_2/patch_embedding_2/conv3d_2/Conv3D (2.47b/2.47b flops)\n  encoder_2/encoder_block_4/dense_20/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_2/encoder_block_4/dense_21/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_2/encoder_block_5/dense_22/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_2/encoder_block_5/dense_23/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_2/encoder_block_4/multi_head_attention_10/softmax/Softmax (12.29m/12.29m flops)\n  encoder_2/encoder_block_5/multi_head_attention_11/softmax_1/Softmax (12.29m/12.29m flops)\n  encoder_2/encoder_block_6/dense_24/Tensordot/MatMul (4.19m/4.19m flops)\n  encoder_2/encoder_block_6/dense_25/Tensordot/MatMul (4.19m/4.19m flops)\n  encoder_2/encoder_block_7/dense_26/Tensordot/MatMul (4.19m/4.19m flops)\n  encoder_2/encoder_block_7/dense_27/Tensordot/MatMul (4.19m/4.19m flops)\n  encoder_2/encoder_block_4/multi_head_attention_10/dropout/dropout/GreaterEqual (2.46m/2.46m flops)\n  encoder_2/encoder_block_4/multi_head_attention_10/dropout/dropout/Mul (2.46m/2.46m flops)\n  encoder_2/encoder_block_5/multi_head_attention_11/dropout_1/dropout/GreaterEqual (2.46m/2.46m flops)\n  encoder_2/encoder_block_5/multi_head_attention_11/dropout_1/dropout/Mul (2.46m/2.46m flops)\n  encoder_2/Mean (802.82k/802.82k flops)\n  encoder_2/add/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/add_1/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/add_2/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/dense_20/BiasAdd (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/dense_20/Gelu/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/dense_20/Gelu/mul (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/dense_20/Gelu/mul_1 (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/dense_20/Gelu/truediv (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/dense_21/BiasAdd (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/dropout_4/dropout/GreaterEqual (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/dropout_4/dropout/Mul (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/layer_normalization_18/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/layer_normalization_18/mul (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/layer_normalization_19/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/layer_normalization_19/mul (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/multi_head_attention_10/Mul (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/multi_head_attention_10/attention_output/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/multi_head_attention_10/key/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/multi_head_attention_10/query/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_4/multi_head_attention_10/value/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/add_3/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/add_4/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/dense_22/BiasAdd (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/dense_22/Gelu/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/dense_22/Gelu/mul (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/dense_22/Gelu/mul_1 (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/dense_22/Gelu/truediv (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/dense_23/BiasAdd (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/dropout_5/dropout/GreaterEqual (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/dropout_5/dropout/Mul (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/layer_normalization_20/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/layer_normalization_20/mul (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/layer_normalization_21/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/layer_normalization_21/mul (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/multi_head_attention_11/Mul (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/multi_head_attention_11/attention_output/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/multi_head_attention_11/key/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/multi_head_attention_11/query/add (802.82k/802.82k flops)\n  encoder_2/encoder_block_5/multi_head_attention_11/value/add (802.82k/802.82k flops)\n  encoder_2/patch_embedding_2/conv3d_2/BiasAdd (802.82k/802.82k flops)\n  encoder_2/encoder_block_6/add_5/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/add_6/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/dense_24/BiasAdd (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/dense_24/Gelu/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/dense_24/Gelu/mul (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/dense_24/Gelu/mul_1 (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/dense_24/Gelu/truediv (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/dense_25/BiasAdd (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/dropout_6/dropout/GreaterEqual (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/dropout_6/dropout/Mul (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/layer_normalization_22/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/layer_normalization_22/mul (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/layer_normalization_23/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/layer_normalization_23/mul (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/multi_head_attention_12/Mul (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/multi_head_attention_12/attention_output/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/multi_head_attention_12/key/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/multi_head_attention_12/query/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/multi_head_attention_12/value/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/add_7/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/add_8/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/dense_26/BiasAdd (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/dense_26/Gelu/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/dense_26/Gelu/mul (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/dense_26/Gelu/mul_1 (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/dense_26/Gelu/truediv (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/dense_27/BiasAdd (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/dropout_7/dropout/GreaterEqual (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/dropout_7/dropout/Mul (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/layer_normalization_24/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/layer_normalization_24/mul (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/layer_normalization_25/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/layer_normalization_25/mul (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/multi_head_attention_13/Mul (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/multi_head_attention_13/attention_output/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/multi_head_attention_13/key/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/multi_head_attention_13/query/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_7/multi_head_attention_13/value/add (4.10k/4.10k flops)\n  encoder_2/encoder_block_6/multi_head_attention_12/softmax_2/Softmax (2.56k/2.56k flops)\n  encoder_2/encoder_block_7/multi_head_attention_13/softmax_3/Softmax (2.56k/2.56k flops)\n  encoder_2/encoder_block_6/multi_head_attention_12/dropout_2/dropout/GreaterEqual (512/512 flops)\n  encoder_2/encoder_block_6/multi_head_attention_12/dropout_2/dropout/Mul (512/512 flops)\n  encoder_2/encoder_block_7/multi_head_attention_13/dropout_3/dropout/GreaterEqual (512/512 flops)\n  encoder_2/encoder_block_7/multi_head_attention_13/dropout_3/dropout/Mul (512/512 flops)\n  encoder_2/mul (1/1 flops)\n\n======================End of Report==========================\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/502.96m flops)\n  decoder_2/dense_33/Tensordot/MatMul (399.36m/399.36m flops)\n  decoder_2/dense_32/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_2/transformer_block_4/dense_28/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_2/transformer_block_4/dense_29/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_2/transformer_block_5/dense_30/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_2/transformer_block_5/dense_31/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_2/dense_33/BiasAdd (390.00k/390.00k flops)\n  decoder_2/multi_head_attention_14/softmax/Softmax (60.84k/60.84k flops)\n  decoder_2/add/add (19.97k/19.97k flops)\n  decoder_2/add_1/add (19.97k/19.97k flops)\n  decoder_2/dense_32/BiasAdd (19.97k/19.97k flops)\n  decoder_2/layer_normalization_26/add (19.97k/19.97k flops)\n  decoder_2/layer_normalization_26/mul (19.97k/19.97k flops)\n  decoder_2/mul (19.97k/19.97k flops)\n  decoder_2/multi_head_attention_14/Mul (19.97k/19.97k flops)\n  decoder_2/multi_head_attention_14/attention_output/add (19.97k/19.97k flops)\n  decoder_2/multi_head_attention_14/key/add (19.97k/19.97k flops)\n  decoder_2/multi_head_attention_14/query/add (19.97k/19.97k flops)\n  decoder_2/multi_head_attention_14/value/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/add_2/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/add_3/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/dense_28/BiasAdd (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/dense_28/Gelu/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/dense_28/Gelu/mul (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/dense_28/Gelu/mul_1 (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/dense_28/Gelu/truediv (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/dense_29/BiasAdd (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/layer_normalization_27/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/layer_normalization_27/mul (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/layer_normalization_28/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/layer_normalization_28/mul (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/Mul (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/attention_output/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/query/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/add_4/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/add_5/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/dense_30/BiasAdd (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/dense_30/Gelu/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/dense_30/Gelu/mul (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/dense_30/Gelu/mul_1 (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/dense_30/Gelu/truediv (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/dense_31/BiasAdd (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/layer_normalization_29/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/layer_normalization_29/mul (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/layer_normalization_30/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/layer_normalization_30/mul (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/Mul (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/attention_output/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/query/add (19.97k/19.97k flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/softmax_1/Softmax (12.48k/12.48k flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/softmax_2/Softmax (12.48k/12.48k flops)\n  decoder_2/multi_head_attention_14/dropout/dropout/GreaterEqual (12.17k/12.17k flops)\n  decoder_2/multi_head_attention_14/dropout/dropout/Mul (12.17k/12.17k flops)\n  decoder_2/multi_head_attention_14/softmax/add (12.17k/12.17k flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/key/add (4.10k/4.10k flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/value/add (4.10k/4.10k flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/key/add (4.10k/4.10k flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/value/add (4.10k/4.10k flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/dropout_1/dropout/GreaterEqual (2.50k/2.50k flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/dropout_1/dropout/Mul (2.50k/2.50k flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/softmax_1/add (2.50k/2.50k flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/dropout_2/dropout/GreaterEqual (2.50k/2.50k flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/dropout_2/dropout/Mul (2.50k/2.50k flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/softmax_2/add (2.50k/2.50k flops)\n  decoder_2/GreaterEqual (1.52k/1.52k flops)\n  decoder_2/Minimum (1.52k/1.52k flops)\n  decoder_2/multi_head_attention_14/softmax/mul (1.52k/1.52k flops)\n  decoder_2/multi_head_attention_14/softmax/sub (1.52k/1.52k flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/softmax_1/mul (39/39 flops)\n  decoder_2/transformer_block_4/multi_head_attention_15/softmax_1/sub (39/39 flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/softmax_2/mul (39/39 flops)\n  decoder_2/transformer_block_5/multi_head_attention_16/softmax_2/sub (39/39 flops)\n\n======================End of Report==========================\nNum of trainable parameters: 21589776\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/5.84b flops)\n  encoder_3/patch_embedding_3/conv3d_3/Conv3D (2.47b/2.47b flops)\n  encoder_3/encoder_block_8/dense_34/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_3/encoder_block_8/dense_35/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_3/encoder_block_9/dense_36/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_3/encoder_block_9/dense_37/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_3/encoder_block_8/multi_head_attention_17/softmax/Softmax (12.29m/12.29m flops)\n  encoder_3/encoder_block_9/multi_head_attention_19/softmax_2/Softmax (12.29m/12.29m flops)\n  encoder_3/encoder_block_8/multi_head_attention_17/dropout/dropout/GreaterEqual (2.46m/2.46m flops)\n  encoder_3/encoder_block_8/multi_head_attention_17/dropout/dropout/Mul (2.46m/2.46m flops)\n  encoder_3/encoder_block_9/multi_head_attention_19/dropout_2/dropout/GreaterEqual (2.46m/2.46m flops)\n  encoder_3/encoder_block_9/multi_head_attention_19/dropout_2/dropout/Mul (2.46m/2.46m flops)\n  encoder_3/add/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/add_1/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/add_2/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/add_3/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/dense_34/BiasAdd (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/dense_34/Gelu/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/dense_34/Gelu/mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/dense_34/Gelu/mul_1 (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/dense_34/Gelu/truediv (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/dense_35/BiasAdd (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/dropout_8/dropout/GreaterEqual (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/dropout_8/dropout/Mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/layer_normalization_31/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/layer_normalization_31/mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/layer_normalization_32/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/layer_normalization_32/mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/layer_normalization_33/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/layer_normalization_33/mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_17/Mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_17/attention_output/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_17/key/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_17/query/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_17/value/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_18/Mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_18/attention_output/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_18/key/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_18/query/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_18/value/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/add_4/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/add_5/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/add_6/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/dense_36/BiasAdd (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/dense_36/Gelu/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/dense_36/Gelu/mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/dense_36/Gelu/mul_1 (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/dense_36/Gelu/truediv (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/dense_37/BiasAdd (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/dropout_9/dropout/GreaterEqual (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/dropout_9/dropout/Mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/layer_normalization_34/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/layer_normalization_34/mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/layer_normalization_35/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/layer_normalization_35/mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/layer_normalization_36/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/layer_normalization_36/mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/multi_head_attention_19/Mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/multi_head_attention_19/attention_output/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/multi_head_attention_19/key/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/multi_head_attention_19/query/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/multi_head_attention_19/value/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/multi_head_attention_20/Mul (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/multi_head_attention_20/attention_output/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/multi_head_attention_20/key/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/multi_head_attention_20/query/add (802.82k/802.82k flops)\n  encoder_3/encoder_block_9/multi_head_attention_20/value/add (802.82k/802.82k flops)\n  encoder_3/patch_embedding_3/conv3d_3/BiasAdd (802.82k/802.82k flops)\n  encoder_3/encoder_block_8/multi_head_attention_18/softmax_1/Softmax (501.76k/501.76k flops)\n  encoder_3/encoder_block_9/multi_head_attention_20/softmax_3/Softmax (501.76k/501.76k flops)\n  encoder_3/encoder_block_8/multi_head_attention_18/dropout_1/dropout/GreaterEqual (100.35k/100.35k flops)\n  encoder_3/encoder_block_8/multi_head_attention_18/dropout_1/dropout/Mul (100.35k/100.35k flops)\n  encoder_3/encoder_block_9/multi_head_attention_20/dropout_3/dropout/GreaterEqual (100.35k/100.35k flops)\n  encoder_3/encoder_block_9/multi_head_attention_20/dropout_3/dropout/Mul (100.35k/100.35k flops)\n  encoder_3/encoder_block_8/mul (1/1 flops)\n  encoder_3/encoder_block_8/mul_1 (1/1 flops)\n  encoder_3/encoder_block_9/mul (1/1 flops)\n  encoder_3/encoder_block_9/mul_1 (1/1 flops)\n\n======================End of Report==========================\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/513.95m flops)\n  decoder_3/dense_43/Tensordot/MatMul (399.36m/399.36m flops)\n  decoder_3/dense_42/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_3/transformer_block_6/dense_38/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_3/transformer_block_6/dense_39/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_3/transformer_block_7/dense_40/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_3/transformer_block_7/dense_41/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/softmax_1/Softmax (2.45m/2.45m flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/softmax_2/Softmax (2.45m/2.45m flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/key/add (802.82k/802.82k flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/value/add (802.82k/802.82k flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/key/add (802.82k/802.82k flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/value/add (802.82k/802.82k flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/dropout_1/dropout/GreaterEqual (489.22k/489.22k flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/dropout_1/dropout/Mul (489.22k/489.22k flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/softmax_1/add (489.22k/489.22k flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/dropout_2/dropout/GreaterEqual (489.22k/489.22k flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/dropout_2/dropout/Mul (489.22k/489.22k flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/softmax_2/add (489.22k/489.22k flops)\n  decoder_3/dense_43/BiasAdd (390.00k/390.00k flops)\n  decoder_3/multi_head_attention_21/softmax/Softmax (60.84k/60.84k flops)\n  decoder_3/add/add (19.97k/19.97k flops)\n  decoder_3/add_1/add (19.97k/19.97k flops)\n  decoder_3/dense_42/BiasAdd (19.97k/19.97k flops)\n  decoder_3/layer_normalization_37/add (19.97k/19.97k flops)\n  decoder_3/layer_normalization_37/mul (19.97k/19.97k flops)\n  decoder_3/mul (19.97k/19.97k flops)\n  decoder_3/multi_head_attention_21/Mul (19.97k/19.97k flops)\n  decoder_3/multi_head_attention_21/attention_output/add (19.97k/19.97k flops)\n  decoder_3/multi_head_attention_21/key/add (19.97k/19.97k flops)\n  decoder_3/multi_head_attention_21/query/add (19.97k/19.97k flops)\n  decoder_3/multi_head_attention_21/value/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/add_2/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/add_3/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/dense_38/BiasAdd (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/dense_38/Gelu/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/dense_38/Gelu/mul (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/dense_38/Gelu/mul_1 (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/dense_38/Gelu/truediv (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/dense_39/BiasAdd (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/layer_normalization_38/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/layer_normalization_38/mul (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/layer_normalization_39/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/layer_normalization_39/mul (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/Mul (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/attention_output/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/query/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/add_4/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/add_5/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/dense_40/BiasAdd (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/dense_40/Gelu/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/dense_40/Gelu/mul (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/dense_40/Gelu/mul_1 (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/dense_40/Gelu/truediv (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/dense_41/BiasAdd (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/layer_normalization_40/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/layer_normalization_40/mul (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/layer_normalization_41/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/layer_normalization_41/mul (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/Mul (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/attention_output/add (19.97k/19.97k flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/query/add (19.97k/19.97k flops)\n  decoder_3/multi_head_attention_21/dropout/dropout/GreaterEqual (12.17k/12.17k flops)\n  decoder_3/multi_head_attention_21/dropout/dropout/Mul (12.17k/12.17k flops)\n  decoder_3/multi_head_attention_21/softmax/add (12.17k/12.17k flops)\n  decoder_3/GreaterEqual (1.52k/1.52k flops)\n  decoder_3/Minimum (1.52k/1.52k flops)\n  decoder_3/multi_head_attention_21/softmax/mul (1.52k/1.52k flops)\n  decoder_3/multi_head_attention_21/softmax/sub (1.52k/1.52k flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/softmax_1/mul (39/39 flops)\n  decoder_3/transformer_block_6/multi_head_attention_22/softmax_1/sub (39/39 flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/softmax_2/mul (39/39 flops)\n  decoder_3/transformer_block_7/multi_head_attention_23/softmax_2/sub (39/39 flops)\n\n======================End of Report==========================\nNum of trainable parameters: 19224336\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/5.80b flops)\n  encoder_4/patch_embedding_4/conv3d_4/Conv3D (2.47b/2.47b flops)\n  encoder_4/encoder_block_10/dense_44/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_4/encoder_block_10/dense_45/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_4/encoder_block_11/dense_46/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_4/encoder_block_11/dense_47/Tensordot/MatMul (822.08m/822.08m flops)\n  encoder_4/encoder_block_10/multi_head_attention_24/softmax/Softmax (6.15m/6.15m flops)\n  encoder_4/encoder_block_11/multi_head_attention_26/softmax_2/Softmax (6.15m/6.15m flops)\n  encoder_4/encoder_block_10/multi_head_attention_24/dropout/dropout/GreaterEqual (1.23m/1.23m flops)\n  encoder_4/encoder_block_10/multi_head_attention_24/dropout/dropout/Mul (1.23m/1.23m flops)\n  encoder_4/encoder_block_11/multi_head_attention_26/dropout_2/dropout/GreaterEqual (1.23m/1.23m flops)\n  encoder_4/encoder_block_11/multi_head_attention_26/dropout_2/dropout/Mul (1.23m/1.23m flops)\n  encoder_4/add/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/Mean (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/Mean_1 (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/add_1/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/add_2/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/dense_44/BiasAdd (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/dense_44/Gelu/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/dense_44/Gelu/mul (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/dense_44/Gelu/mul_1 (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/dense_44/Gelu/truediv (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/dense_45/BiasAdd (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/dropout_10/dropout/GreaterEqual (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/dropout_10/dropout/Mul (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/layer_normalization_42/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/layer_normalization_42/mul (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/layer_normalization_43/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/layer_normalization_43/mul (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/Mean (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/Mean_1 (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/add_3/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/add_4/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/dense_46/BiasAdd (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/dense_46/Gelu/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/dense_46/Gelu/mul (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/dense_46/Gelu/mul_1 (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/dense_46/Gelu/truediv (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/dense_47/BiasAdd (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/dropout_11/dropout/GreaterEqual (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/dropout_11/dropout/Mul (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/layer_normalization_44/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/layer_normalization_44/mul (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/layer_normalization_45/add (802.82k/802.82k flops)\n  encoder_4/encoder_block_11/layer_normalization_45/mul (802.82k/802.82k flops)\n  encoder_4/patch_embedding_4/conv3d_4/BiasAdd (802.82k/802.82k flops)\n  encoder_4/encoder_block_10/multi_head_attention_24/Mul (401.41k/401.41k flops)\n  encoder_4/encoder_block_10/multi_head_attention_24/attention_output/add (401.41k/401.41k flops)\n  encoder_4/encoder_block_10/multi_head_attention_24/query/add (401.41k/401.41k flops)\n  encoder_4/encoder_block_10/multi_head_attention_25/Mul (401.41k/401.41k flops)\n  encoder_4/encoder_block_10/multi_head_attention_25/attention_output/add (401.41k/401.41k flops)\n  encoder_4/encoder_block_10/multi_head_attention_25/query/add (401.41k/401.41k flops)\n  encoder_4/encoder_block_11/multi_head_attention_26/Mul (401.41k/401.41k flops)\n  encoder_4/encoder_block_11/multi_head_attention_26/attention_output/add (401.41k/401.41k flops)\n  encoder_4/encoder_block_11/multi_head_attention_26/query/add (401.41k/401.41k flops)\n  encoder_4/encoder_block_11/multi_head_attention_27/Mul (401.41k/401.41k flops)\n  encoder_4/encoder_block_11/multi_head_attention_27/attention_output/add (401.41k/401.41k flops)\n  encoder_4/encoder_block_11/multi_head_attention_27/query/add (401.41k/401.41k flops)\n  encoder_4/encoder_block_10/multi_head_attention_25/softmax_1/Softmax (250.88k/250.88k flops)\n  encoder_4/encoder_block_11/multi_head_attention_27/softmax_3/Softmax (250.88k/250.88k flops)\n  encoder_4/encoder_block_10/multi_head_attention_24/key/add (50.18k/50.18k flops)\n  encoder_4/encoder_block_10/multi_head_attention_24/value/add (50.18k/50.18k flops)\n  encoder_4/encoder_block_10/multi_head_attention_25/dropout_1/dropout/GreaterEqual (50.18k/50.18k flops)\n  encoder_4/encoder_block_10/multi_head_attention_25/dropout_1/dropout/Mul (50.18k/50.18k flops)\n  encoder_4/encoder_block_11/multi_head_attention_26/key/add (50.18k/50.18k flops)\n  encoder_4/encoder_block_11/multi_head_attention_26/value/add (50.18k/50.18k flops)\n  encoder_4/encoder_block_11/multi_head_attention_27/dropout_3/dropout/GreaterEqual (50.18k/50.18k flops)\n  encoder_4/encoder_block_11/multi_head_attention_27/dropout_3/dropout/Mul (50.18k/50.18k flops)\n  encoder_4/encoder_block_10/multi_head_attention_25/key/add (2.05k/2.05k flops)\n  encoder_4/encoder_block_10/multi_head_attention_25/value/add (2.05k/2.05k flops)\n  encoder_4/encoder_block_11/multi_head_attention_27/key/add (2.05k/2.05k flops)\n  encoder_4/encoder_block_11/multi_head_attention_27/value/add (2.05k/2.05k flops)\n\n======================End of Report==========================\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/513.95m flops)\n  decoder_4/dense_53/Tensordot/MatMul (399.36m/399.36m flops)\n  decoder_4/dense_52/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_4/transformer_block_8/dense_48/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_4/transformer_block_8/dense_49/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_4/transformer_block_9/dense_50/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_4/transformer_block_9/dense_51/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/softmax_1/Softmax (2.45m/2.45m flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/softmax_2/Softmax (2.45m/2.45m flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/key/add (802.82k/802.82k flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/value/add (802.82k/802.82k flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/key/add (802.82k/802.82k flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/value/add (802.82k/802.82k flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/dropout_1/dropout/GreaterEqual (489.22k/489.22k flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/dropout_1/dropout/Mul (489.22k/489.22k flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/softmax_1/add (489.22k/489.22k flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/dropout_2/dropout/GreaterEqual (489.22k/489.22k flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/dropout_2/dropout/Mul (489.22k/489.22k flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/softmax_2/add (489.22k/489.22k flops)\n  decoder_4/dense_53/BiasAdd (390.00k/390.00k flops)\n  decoder_4/multi_head_attention_28/softmax/Softmax (60.84k/60.84k flops)\n  decoder_4/add/add (19.97k/19.97k flops)\n  decoder_4/add_1/add (19.97k/19.97k flops)\n  decoder_4/dense_52/BiasAdd (19.97k/19.97k flops)\n  decoder_4/layer_normalization_46/add (19.97k/19.97k flops)\n  decoder_4/layer_normalization_46/mul (19.97k/19.97k flops)\n  decoder_4/mul (19.97k/19.97k flops)\n  decoder_4/multi_head_attention_28/Mul (19.97k/19.97k flops)\n  decoder_4/multi_head_attention_28/attention_output/add (19.97k/19.97k flops)\n  decoder_4/multi_head_attention_28/key/add (19.97k/19.97k flops)\n  decoder_4/multi_head_attention_28/query/add (19.97k/19.97k flops)\n  decoder_4/multi_head_attention_28/value/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/add_2/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/add_3/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/dense_48/BiasAdd (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/dense_48/Gelu/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/dense_48/Gelu/mul (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/dense_48/Gelu/mul_1 (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/dense_48/Gelu/truediv (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/dense_49/BiasAdd (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/layer_normalization_47/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/layer_normalization_47/mul (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/layer_normalization_48/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/layer_normalization_48/mul (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/Mul (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/attention_output/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/query/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/add_4/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/add_5/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/dense_50/BiasAdd (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/dense_50/Gelu/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/dense_50/Gelu/mul (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/dense_50/Gelu/mul_1 (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/dense_50/Gelu/truediv (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/dense_51/BiasAdd (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/layer_normalization_49/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/layer_normalization_49/mul (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/layer_normalization_50/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/layer_normalization_50/mul (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/Mul (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/attention_output/add (19.97k/19.97k flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/query/add (19.97k/19.97k flops)\n  decoder_4/multi_head_attention_28/dropout/dropout/GreaterEqual (12.17k/12.17k flops)\n  decoder_4/multi_head_attention_28/dropout/dropout/Mul (12.17k/12.17k flops)\n  decoder_4/multi_head_attention_28/softmax/add (12.17k/12.17k flops)\n  decoder_4/GreaterEqual (1.52k/1.52k flops)\n  decoder_4/Minimum (1.52k/1.52k flops)\n  decoder_4/multi_head_attention_28/softmax/mul (1.52k/1.52k flops)\n  decoder_4/multi_head_attention_28/softmax/sub (1.52k/1.52k flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/softmax_1/mul (39/39 flops)\n  decoder_4/transformer_block_8/multi_head_attention_29/softmax_1/sub (39/39 flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/softmax_2/mul (39/39 flops)\n  decoder_4/transformer_block_9/multi_head_attention_30/softmax_2/sub (39/39 flops)\n\n======================End of Report==========================\nNum of trainable parameters: 26760362\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/1.34b flops)\n  encoder_5/patch_embedding_5/conv3d_9/Conv3D (616.56m/616.56m flops)\n  encoder_5/dense_59/Tensordot/MatMul (106.95m/106.95m flops)\n  encoder_5/encoder_block_12/dense_55/Tensordot_1/MatMul (102.76m/102.76m flops)\n  encoder_5/encoder_block_12/dense_56/Tensordot_1/MatMul (102.76m/102.76m flops)\n  encoder_5/encoder_block_13/dense_57/Tensordot_1/MatMul (102.76m/102.76m flops)\n  encoder_5/encoder_block_13/dense_58/Tensordot_1/MatMul (102.76m/102.76m flops)\n  encoder_5/dense_54/Tensordot/MatMul (77.07m/77.07m flops)\n  encoder_5/conv3d_5/Conv3D (43.35m/43.35m flops)\n  encoder_5/conv3d_7/Conv3D (28.90m/28.90m flops)\n  encoder_5/conv3d_8/Conv3D (14.45m/14.45m flops)\n  encoder_5/conv3d_6/Conv3D (10.84m/10.84m flops)\n  encoder_5/encoder_block_12/dense_55/Tensordot/MatMul (4.19m/4.19m flops)\n  encoder_5/encoder_block_12/dense_56/Tensordot/MatMul (4.19m/4.19m flops)\n  encoder_5/encoder_block_13/dense_57/Tensordot/MatMul (4.19m/4.19m flops)\n  encoder_5/encoder_block_13/dense_58/Tensordot/MatMul (4.19m/4.19m flops)\n  encoder_5/conv3d_7/BiasAdd (1.61m/1.61m flops)\n  encoder_5/encoder_block_12/multi_head_attention_32/softmax_1/Softmax (1.60m/1.60m flops)\n  encoder_5/encoder_block_13/multi_head_attention_34/softmax_3/Softmax (1.60m/1.60m flops)\n  encoder_5/conv3d_5/BiasAdd (802.82k/802.82k flops)\n  encoder_5/conv3d_8/BiasAdd (602.11k/602.11k flops)\n  encoder_5/encoder_block_12/multi_head_attention_32/dropout_1/dropout/GreaterEqual (319.87k/319.87k flops)\n  encoder_5/encoder_block_12/multi_head_attention_32/dropout_1/dropout/Mul (319.87k/319.87k flops)\n  encoder_5/encoder_block_13/multi_head_attention_34/dropout_3/dropout/GreaterEqual (319.87k/319.87k flops)\n  encoder_5/encoder_block_13/multi_head_attention_34/dropout_3/dropout/Mul (319.87k/319.87k flops)\n  encoder_5/dense_59/BiasAdd (104.45k/104.45k flops)\n  encoder_5/dense_59/Gelu/add (104.45k/104.45k flops)\n  encoder_5/dense_59/Gelu/mul (104.45k/104.45k flops)\n  encoder_5/dense_59/Gelu/mul_1 (104.45k/104.45k flops)\n  encoder_5/dense_59/Gelu/truediv (104.45k/104.45k flops)\n  encoder_5/encoder_block_12/multi_head_attention_31/key/add (104.45k/104.45k flops)\n  encoder_5/encoder_block_12/multi_head_attention_31/value/add (104.45k/104.45k flops)\n  encoder_5/encoder_block_12/multi_head_attention_32/key/add (104.45k/104.45k flops)\n  encoder_5/encoder_block_12/multi_head_attention_32/value/add (104.45k/104.45k flops)\n  encoder_5/encoder_block_13/multi_head_attention_33/key/add (104.45k/104.45k flops)\n  encoder_5/encoder_block_13/multi_head_attention_33/value/add (104.45k/104.45k flops)\n  encoder_5/encoder_block_13/multi_head_attention_34/key/add (104.45k/104.45k flops)\n  encoder_5/encoder_block_13/multi_head_attention_34/value/add (104.45k/104.45k flops)\n  encoder_5/add_1/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/add_3/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/add_5/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/dense_55/BiasAdd_1 (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/dense_55/Gelu_1/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/dense_55/Gelu_1/mul (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/dense_55/Gelu_1/mul_1 (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/dense_55/Gelu_1/truediv (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/dense_56/BiasAdd_1 (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/dropout_12/dropout_1/GreaterEqual (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/dropout_12/dropout_1/Mul (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/layer_normalization_52/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/layer_normalization_52/mul (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/layer_normalization_54/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/layer_normalization_54/mul (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/multi_head_attention_32/Mul (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/multi_head_attention_32/attention_output/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_12/multi_head_attention_32/query/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/add_7/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/add_9/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/dense_57/BiasAdd_1 (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/dense_57/Gelu_1/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/dense_57/Gelu_1/mul (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/dense_57/Gelu_1/mul_1 (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/dense_57/Gelu_1/truediv (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/dense_58/BiasAdd_1 (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/dropout_13/dropout_1/GreaterEqual (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/dropout_13/dropout_1/Mul (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/layer_normalization_56/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/layer_normalization_56/mul (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/layer_normalization_58/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/layer_normalization_58/mul (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/multi_head_attention_34/Mul (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/multi_head_attention_34/attention_output/add (100.35k/100.35k flops)\n  encoder_5/encoder_block_13/multi_head_attention_34/query/add (100.35k/100.35k flops)\n  encoder_5/patch_embedding_5/conv3d_9/BiasAdd (100.35k/100.35k flops)\n  encoder_5/conv3d_6/BiasAdd (75.26k/75.26k flops)\n  encoder_5/encoder_block_12/multi_head_attention_31/softmax/Softmax (65.28k/65.28k flops)\n  encoder_5/encoder_block_13/multi_head_attention_33/softmax_2/Softmax (65.28k/65.28k flops)\n  encoder_5/encoder_block_12/multi_head_attention_31/dropout/dropout/GreaterEqual (13.06k/13.06k flops)\n  encoder_5/encoder_block_12/multi_head_attention_31/dropout/dropout/Mul (13.06k/13.06k flops)\n  encoder_5/encoder_block_13/multi_head_attention_33/dropout_2/dropout/GreaterEqual (13.06k/13.06k flops)\n  encoder_5/encoder_block_13/multi_head_attention_33/dropout_2/dropout/Mul (13.06k/13.06k flops)\n  encoder_5/add/add (4.10k/4.10k flops)\n  encoder_5/dense_54/BiasAdd (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/add_2/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/add_4/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/dense_55/BiasAdd (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/dense_55/Gelu/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/dense_55/Gelu/mul (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/dense_55/Gelu/mul_1 (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/dense_55/Gelu/truediv (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/dense_56/BiasAdd (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/dropout_12/dropout/GreaterEqual (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/dropout_12/dropout/Mul (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/layer_normalization_51/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/layer_normalization_51/mul (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/layer_normalization_53/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/layer_normalization_53/mul (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/multi_head_attention_31/Mul (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/multi_head_attention_31/attention_output/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_12/multi_head_attention_31/query/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/add_6/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/add_8/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/dense_57/BiasAdd (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/dense_57/Gelu/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/dense_57/Gelu/mul (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/dense_57/Gelu/mul_1 (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/dense_57/Gelu/truediv (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/dense_58/BiasAdd (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/dropout_13/dropout/GreaterEqual (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/dropout_13/dropout/Mul (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/layer_normalization_55/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/layer_normalization_55/mul (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/layer_normalization_57/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/layer_normalization_57/mul (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/multi_head_attention_33/Mul (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/multi_head_attention_33/attention_output/add (4.10k/4.10k flops)\n  encoder_5/encoder_block_13/multi_head_attention_33/query/add (4.10k/4.10k flops)\n\n======================End of Report==========================\n\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/504.34m flops)\n  decoder_5/dense_65/Tensordot/MatMul (399.36m/399.36m flops)\n  decoder_5/dense_64/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_5/transformer_block_10/dense_60/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_5/transformer_block_10/dense_61/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_5/transformer_block_11/dense_62/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_5/transformer_block_11/dense_63/Tensordot/MatMul (20.45m/20.45m flops)\n  decoder_5/dense_65/BiasAdd (390.00k/390.00k flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/softmax_1/Softmax (318.24k/318.24k flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/softmax_2/Softmax (318.24k/318.24k flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/key/add (104.45k/104.45k flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/value/add (104.45k/104.45k flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/key/add (104.45k/104.45k flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/value/add (104.45k/104.45k flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/dropout_1/dropout/GreaterEqual (63.65k/63.65k flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/dropout_1/dropout/Mul (63.65k/63.65k flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/softmax_1/add (63.65k/63.65k flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/dropout_2/dropout/GreaterEqual (63.65k/63.65k flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/dropout_2/dropout/Mul (63.65k/63.65k flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/softmax_2/add (63.65k/63.65k flops)\n  decoder_5/multi_head_attention_35/softmax/Softmax (60.84k/60.84k flops)\n  decoder_5/add/add (19.97k/19.97k flops)\n  decoder_5/add_1/add (19.97k/19.97k flops)\n  decoder_5/dense_64/BiasAdd (19.97k/19.97k flops)\n  decoder_5/layer_normalization_59/add (19.97k/19.97k flops)\n  decoder_5/layer_normalization_59/mul (19.97k/19.97k flops)\n  decoder_5/mul (19.97k/19.97k flops)\n  decoder_5/multi_head_attention_35/Mul (19.97k/19.97k flops)\n  decoder_5/multi_head_attention_35/attention_output/add (19.97k/19.97k flops)\n  decoder_5/multi_head_attention_35/key/add (19.97k/19.97k flops)\n  decoder_5/multi_head_attention_35/query/add (19.97k/19.97k flops)\n  decoder_5/multi_head_attention_35/value/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/add_2/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/add_3/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/dense_60/BiasAdd (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/dense_60/Gelu/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/dense_60/Gelu/mul (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/dense_60/Gelu/mul_1 (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/dense_60/Gelu/truediv (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/dense_61/BiasAdd (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/layer_normalization_60/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/layer_normalization_60/mul (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/layer_normalization_61/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/layer_normalization_61/mul (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/Mul (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/attention_output/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/query/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/add_4/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/add_5/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/dense_62/BiasAdd (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/dense_62/Gelu/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/dense_62/Gelu/mul (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/dense_62/Gelu/mul_1 (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/dense_62/Gelu/truediv (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/dense_63/BiasAdd (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/layer_normalization_62/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/layer_normalization_62/mul (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/layer_normalization_63/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/layer_normalization_63/mul (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/Mul (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/attention_output/add (19.97k/19.97k flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/query/add (19.97k/19.97k flops)\n  decoder_5/multi_head_attention_35/dropout/dropout/GreaterEqual (12.17k/12.17k flops)\n  decoder_5/multi_head_attention_35/dropout/dropout/Mul (12.17k/12.17k flops)\n  decoder_5/multi_head_attention_35/softmax/add (12.17k/12.17k flops)\n  decoder_5/GreaterEqual (1.52k/1.52k flops)\n  decoder_5/Minimum (1.52k/1.52k flops)\n  decoder_5/multi_head_attention_35/softmax/mul (1.52k/1.52k flops)\n  decoder_5/multi_head_attention_35/softmax/sub (1.52k/1.52k flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/softmax_1/mul (39/39 flops)\n  decoder_5/transformer_block_10/multi_head_attention_36/softmax_1/sub (39/39 flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/softmax_2/mul (39/39 flops)\n  decoder_5/transformer_block_11/multi_head_attention_37/softmax_2/sub (39/39 flops)\n\n======================End of Report==========================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(f\"SpaTem Runtime: {ms1:.3f} ms, FLOPs: {flops1:.3f} GFLOPs\")\nprint(f\"FactEnc Runtime: {ms2:.3f} ms, FLOPs: {flops2:.3f} GFLOPs\")\nprint(f\"FactSAtt Runtime: {ms3:.3f} ms, FLOPs: {flops3:.3f} GFLOPs\")\nprint(f\"FactDPro Runtime: {ms4:.3f} ms, FLOPs: {flops4:.3f} GFLOPs\")\nprint(f\"CAtt Runtime: {ms5:.3f} ms, FLOPs: {flops5:.3f} GFLOPs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T20:35:42.429509Z","iopub.execute_input":"2025-03-29T20:35:42.429821Z","iopub.status.idle":"2025-03-29T20:35:42.435838Z","shell.execute_reply.started":"2025-03-29T20:35:42.429795Z","shell.execute_reply":"2025-03-29T20:35:42.434721Z"}},"outputs":[{"name":"stdout","text":"SpaTem Runtime: 19.103 ms, FLOPs: 6.576 GFLOPs\nFactEnc Runtime: 8.578 ms, FLOPs: 6.342 GFLOPs\nFactSAtt Runtime: 12.733 ms, FLOPs: 6.349 GFLOPs\nFactDPro Runtime: 8.090 ms, FLOPs: 6.319 GFLOPs\nCAtt Runtime: 5.294 ms, FLOPs: 1.843 GFLOPs\n","output_type":"stream"}],"execution_count":9}]}