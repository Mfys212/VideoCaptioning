My research (thesis Bachelor's degree) creates a new ViViT-based video feature extraction architecture that has 3.5x lower GFLOPs by reducing the quadratic complexity of self-attention compared to Factorized Dot-Product Attention (the lightest ViViT model) with comparable performance with all ViViT models based on CIDEr scores in video captioning tasks.

cite the original paper:
```
@misc{arnab2021vivitvideovisiontransformer,
      title={ViViT: A Video Vision Transformer}, 
      author={Anurag Arnab and Mostafa Dehghani and Georg Heigold and Chen Sun and Mario Lučić and Cordelia Schmid},
      year={2021},
      eprint={2103.15691},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.15691}, 
}
```
